"""
Portable Project Setup Script
=============================
This script recreates the entire project file structure and writes the file contents.
Generated automatically.

Usage:
    python setup.py
"""

import os
import sys
from pathlib import Path

# ============================================================================
# PROJECT CONTENT
# ============================================================================
# Dictionary mapping file paths to their contents
FILE_CONTENTS = {
  ".env": "API_ENDPOINT=\nAPI_KEY=\"testkey\"\n\nMODEL_CHAT_BASIC=\nMODEL_CHAT_MOD=\nMODEL_CHAT_OPEN=\nMODEL_REASONING=\nMODEL_VISION=\nMODEL_EMBEDDING=\nMODEL_AUDIO=\n\nsecret_key=\"mysecretkey\"\ndatabase_url=\"sqlite:///./my_database.db\"\nSERPER_API_KEY=\"\"",
  ".gitignore": ".venv\n__pycache__\n*/__pycache__/*\nlogs/\nuploads\nuv.lock\n.my_database.db",
  "main.py": "\"\"\"\nMain entry point for the application\n\"\"\"\n\nimport uvicorn\nfrom app.app import create_app\nfrom app.core.config import settings\n\napp = create_app()\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"main:app\",\n        host=\"0.0.0.0\",\n        port=8001,\n        reload=settings.debug,\n        log_level=\"info\",\n    )\n",
  "pyproject.toml": "\n\n[project]\nname = \"agentic-fastapi\"\nversion = \"0.1.0\"\ndescription = \"FastAPI application with agentic behavior using LangGraph\"\nauthors = [\n    {name = \"Your Name\", email = \"your.email@example.com\"}\n]\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nlicense = \"MIT\"\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Developers\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Topic :: Internet :: WWW/HTTP :: Dynamic Content\",\n]\n\ndependencies = [\n    \"fastapi[standard]\",\n    \"uvicorn\",\n    \"pydantic[email]>=2.7.4,<3.0.0\",\n    \"pydantic-settings>=2.3.0,<3.0.0\",\n    \"sqlalchemy\",\n    \"python-dotenv==1.0.0\",\n    \"python-multipart\",\n    \"langgraph\",\n    \"langchain\",\n    \"openai\",\n    \"google-cloud-aiplatform\",\n    \"pyjwt==2.8.0\",\n    \"bcrypt>=5.0.0\",\n    \"langchain-google-genai\",\n    \"langchain-openai\",\n    \"langchain_mcp_adapters\",\n    # Observability dependencies\n    \"opentelemetry-api>=1.20.0\",\n    \"opentelemetry-sdk>=1.20.0\",\n    \"opentelemetry-instrumentation>=0.41b0\",\n    \"traceloop-sdk>=0.15.0\",\n    \"arize-phoenix>=4.0.0\",\n    \"pypdf>=6.4.0\",\n    \"chromadb>=1.3.5\",\n    \"langchain-community>=0.4.1\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest==7.4.3\",\n    \"pytest-asyncio==0.21.1\",\n    \"black==23.12.0\",\n    \"flake8==6.1.0\",\n    \"mypy==1.7.0\",\n    \"isort==5.13.2\",\n    \"pytest-cov\"\n]\n",
  "requirements.txt": "fastapi==0.104.1\nuvicorn==0.24.0\npydantic==2.5.0\npydantic[email]==2.5.0\npydantic-settings==2.1.0\nsqlalchemy==2.0.23\npython-dotenv==1.0.0\npython-multipart==0.0.6\nlanggraph==0.0.18\nlangchain==0.1.0\nlangchain-openai==0.0.5\nopenai==1.3.0\npyjwt==2.8.0\npasslib[bcrypt]==1.7.4\nbcrypt==4.1.0\npytest==7.4.3\npytest-asyncio==0.21.1\nhttpx==0.25.0\n",
  "app/app.py": "\"\"\"\nFastAPI Application Factory with Vertical Feature Architecture\n\"\"\"\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\nfrom app.core.config import settings\nfrom app.core.database import init_db, close_db\nfrom app.core.config.observability_config import initialize_observability, shutdown_observability\nfrom app.middleware import AuthMiddleware\nfrom app.features import (\n    auth_router,\n    users_router,\n    chat_router,\n    documents_router,\n)\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan context manager.\"\"\"\n    # Startup\n    logger.info(f\"Starting {settings.app_name}\")\n    \n    # Initialize database\n    init_db()\n    \n    # Initialize observability (Phoenix + OpenTelemetry)\n    initialize_observability()\n\n    yield\n\n    # Shutdown\n    logger.info(f\"Shutting down {settings.app_name}\")\n    \n    # Shutdown observability\n    shutdown_observability()\n    \n    # Close database\n    close_db()\n\n\ndef create_app() -> FastAPI:\n    \"\"\"\n    Create and configure FastAPI application.\n\n    Returns:\n        Configured FastAPI instance\n    \"\"\"\n    app = FastAPI(\n        title=settings.api_title,\n        description=settings.api_description,\n        version=settings.app_version,\n        debug=settings.debug,\n        lifespan=lifespan,\n    )\n\n    # Add CORS middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n        expose_headers=[\"*\"]\n    )\n\n    # Add authentication middleware\n    app.add_middleware(AuthMiddleware)\n\n    # Include routers\n    app.include_router(auth_router, prefix=settings.api_prefix)\n    app.include_router(users_router, prefix=settings.api_prefix)\n    app.include_router(chat_router, prefix=settings.api_prefix)\n    app.include_router(documents_router, prefix=settings.api_prefix)\n\n    logger.info(\"FastAPI application created successfully\")\n    return app\n",
  "app/__init__.py": "\"\"\"\nMain application package\n\"\"\"\n",
  "app/core/__init__.py": "\"\"\"\nCore Module - Shared infrastructure (config, database, logging, utilities)\n\"\"\"\n\nfrom .config import settings\nfrom .database import init_db, close_db, get_db\nfrom .base.entity import Base\nfrom .utils import get_logger\n\n__all__ = [\"settings\", \"init_db\", \"close_db\", \"get_db\", \"Base\", \"get_logger\"]\n",
  "app/core/base/entity.py": "\"\"\"\nBase Entity - All database entities inherit from this\n\"\"\"\n\nfrom sqlalchemy import Column, DateTime\nfrom sqlalchemy.orm import declarative_base\nfrom datetime import datetime\n\nBase = declarative_base()\n\n\nclass BaseEntity(Base):\n    \"\"\"Base entity class for all database models.\"\"\"\n\n    __abstract__ = True\n\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)\n",
  "app/core/base/repository.py": "\"\"\"\nBase Repository - Generic CRUD operations with session management\n\"\"\"\n\nfrom typing import TypeVar, Generic, List, Optional\nfrom sqlalchemy.orm import Session\nfrom app.core.database import SessionLocal\n\nT = TypeVar(\"T\")\n\n\nclass BaseRepository(Generic[T]):\n    \"\"\"Generic repository for basic CRUD operations with session management.\"\"\"\n\n    def __init__(self, model: type[T]):\n        self.model = model\n        self.db: Session = None\n\n    def _get_db(self) -> Session:\n        \"\"\"Get or create database session.\"\"\"\n        if self.db is None:\n            self.db = SessionLocal()\n        return self.db\n\n    def create(self, obj: T) -> T:\n        \"\"\"Create and commit a new object.\"\"\"\n        db = self._get_db()\n        db.add(obj)\n        db.commit()\n        db.refresh(obj)\n        return obj\n\n    def get_by_id(self, obj_id: int) -> Optional[T]:\n        \"\"\"Get object by ID.\"\"\"\n        db = self._get_db()\n        return db.query(self.model).filter(self.model.id == obj_id).first()\n\n    def get_all(self, skip: int = 0, limit: int = 100) -> List[T]:\n        \"\"\"Get all objects with pagination.\"\"\"\n        db = self._get_db()\n        return db.query(self.model).offset(skip).limit(limit).all()\n\n    def update(self, obj: T) -> T:\n        \"\"\"Update and commit an object.\"\"\"\n        db = self._get_db()\n        db.merge(obj)\n        db.commit()\n        return obj\n\n    def delete(self, obj_id: int) -> bool:\n        \"\"\"Delete an object by ID.\"\"\"\n        db = self._get_db()\n        obj = db.query(self.model).filter(self.model.id == obj_id).first()\n        if obj:\n            db.delete(obj)\n            db.commit()\n            return True\n        return False\n\n    def close(self):\n        \"\"\"Close database session.\"\"\"\n        if self.db:\n            self.db.close()\n            self.db = None\n\n    def __del__(self):\n        \"\"\"Cleanup on deletion.\"\"\"\n        self.close()\n",
  "app/core/base/__init__.py": "\"\"\"\nCore Base Module - Base classes for entities and repositories\n\"\"\"\n\nfrom .entity import Base, BaseEntity\nfrom .repository import BaseRepository\n\n__all__ = [\"Base\", \"BaseEntity\", \"BaseRepository\"]\n",
  "app/core/config/observability_config.py": "\"\"\"\nObservability configuration and initialization.\n\nSets up OpenTelemetry and Phoenix for LLM observability.\n\"\"\"\n\nimport os\nfrom app.core.config import settings\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef initialize_observability():\n    \"\"\"\n    Initialize observability infrastructure.\n    \n    Sets up:\n    - OpenTelemetry tracer\n    - Phoenix integration\n    - Traceloop SDK for LLM instrumentation\n    \"\"\"\n    if not settings.enable_observability:\n        logger.info(\"Observability is disabled\")\n        return\n    \n    try:\n        logger.info(\"Initializing observability infrastructure...\")\n        \n        # Import dependencies only if observability is enabled\n        try:\n            from opentelemetry import trace\n            from opentelemetry.sdk.trace import TracerProvider\n            from opentelemetry.sdk.trace.export import BatchSpanProcessor\n            from opentelemetry.sdk.resources import Resource, SERVICE_NAME\n            from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n            import phoenix as px\n            from traceloop.sdk import Traceloop\n        except ImportError as e:\n            logger.error(f\"Failed to import observability dependencies: {str(e)}\")\n            logger.warning(\"Continuing without observability...\")\n            return\n        \n        # Initialize Phoenix\n        logger.info(f\"Starting Phoenix on {settings.phoenix_host}:{settings.phoenix_port}\")\n        try:\n            px.launch_app(host=settings.phoenix_host, port=settings.phoenix_port)\n        except Exception as e:\n            logger.error(f\"Failed to launch Phoenix: {str(e)}\")\n            logger.warning(\"Continuing without Phoenix UI...\")\n            # Continue anyway - we can still use OpenTelemetry without Phoenix\n        \n        # Set up OpenTelemetry resource\n        resource = Resource(attributes={\n            SERVICE_NAME: settings.app_name,\n            \"service.version\": settings.app_version,\n            \"deployment.environment\": settings.environment,\n        })\n        \n        # Create tracer provider\n        tracer_provider = TracerProvider(resource=resource)\n        \n        # Configure Phoenix as the OTLP endpoint\n        phoenix_endpoint = f\"{settings.phoenix_collector_endpoint}/v1/traces\"\n        otlp_exporter = OTLPSpanExporter(endpoint=phoenix_endpoint)\n        \n        # Add span processor\n        span_processor = BatchSpanProcessor(otlp_exporter)\n        tracer_provider.add_span_processor(span_processor)\n        \n        # Set as global tracer provider\n        trace.set_tracer_provider(tracer_provider)\n        \n        # Initialize Traceloop SDK for automatic LLM instrumentation\n        try:\n            Traceloop.init(\n                app_name=settings.app_name,\n                disable_batch=False,\n                exporter_endpoint=phoenix_endpoint,\n            )\n            logger.info(\"\u2705 Traceloop SDK initialized\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize Traceloop: {str(e)}\")\n            logger.warning(\"Continuing without automatic LLM instrumentation...\")\n        \n        logger.info(\"\u2705 Observability initialized successfully\")\n        logger.info(f\"\ud83d\udcca Phoenix UI available at: http://{settings.phoenix_host}:{settings.phoenix_port}\")\n        \n    except Exception as e:\n        logger.error(f\"Failed to initialize observability: {str(e)}\", exc_info=True)\n        logger.warning(\"Continuing without observability...\")\n\n\ndef shutdown_observability():\n    \"\"\"\n    Gracefully shutdown observability infrastructure.\n    \"\"\"\n    if not settings.enable_observability:\n        return\n    \n    try:\n        logger.info(\"Shutting down observability...\")\n        \n        try:\n            from opentelemetry import trace\n            \n            # Flush any pending spans\n            tracer_provider = trace.get_tracer_provider()\n            if hasattr(tracer_provider, 'shutdown'):\n                tracer_provider.shutdown()\n        except Exception as e:\n            logger.error(f\"Error flushing traces: {str(e)}\")\n        \n        logger.info(\"\u2705 Observability shutdown complete\")\n        \n    except Exception as e:\n        logger.error(f\"Error during observability shutdown: {str(e)}\")\n",
  "app/core/config/settings.py": "\"\"\"\nApplication settings and configuration\n\"\"\"\n\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\nimport os\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application configuration settings.\"\"\"\n\n    # Application settings\n    app_name: str = \"Chat Application\"\n    app_version: str = \"0.1.0\"\n    debug: bool = True\n    environment: str = \"development\"\n\n    # API settings\n    api_prefix: str = \"/api/v1\"\n    api_title: str = \"Chat Application API\"\n    api_description: str = \"FastAPI Chat Application with Agent Pipeline\"\n\n    # Database settings\n    database_url: str = \"\"\n    echo_sql: bool = False\n\n    # LLM settings\n    API_ENDPOINT: str = \"\"\n    API_KEY: str = \"\"\n    MODEL_CHAT_BASIC: str = \"\"\n    MODEL_CHAT_MOD: str = \"\"\n    MODEL_CHAT_OPEN: str = \"\"\n    MODEL_REASONING: str = \"\"\n    MODEL_VISION: str = \"\"\n    MODEL_EMBEDDING: str = \"\"\n    MODEL_AUDIO: str = \"\"\n\n    # Embedding settings\n    embedding_provider: str = \"openai\"\n    embedding_model: str = \"text-embedding-3-small\"\n\n    # Authentication settings\n    secret_key: str = \"\"\n    algorithm: str = \"HS256\"\n    access_token_expire_minutes: int = 30\n\n    # LangGraph settings\n    max_iterations: int = 10\n    timeout: int = 300\n\n    # File upload settings\n    upload_directory: str = \"./uploads\"\n    max_upload_size: int = 10 * 1024 * 1024  # 10MB\n    allowed_file_types: list = [\".pdf\", \".txt\", \".doc\", \".docx\"]\n\n    # Logging settings\n    log_directory: str = \"./logs\"\n    log_level: str = \"INFO\"\n\n\n    # CORS settings\n    cors_origins: list = [\"*\"]\n    cors_credentials: bool = True\n    cors_methods: list = [\"*\"]\n    cors_headers: list = [\"*\"]\n\n    # Excluded routes from authentication\n    auth_excluded_routes: list = [\"/health\", \"/api/v1/auth/login\",\"/api/v1/chat/ws\"]\n\n    # Observability settings\n    enable_observability: bool = True\n    phoenix_host: str = \"localhost\"\n    phoenix_port: int = 6006\n    trace_retention_days: int = 7\n    enable_pii_redaction: bool = False  # Set to True in production\n    observability_sample_rate: float = 1.0  # 1.0 = 100% of requests\n    phoenix_collector_endpoint: str = \"http://localhost:6006\"\n    SERPER_API_KEY:str=\"\"\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n\n\n# Global settings instance\nsettings = Settings()\n\n# Ensure directories exist\nos.makedirs(settings.log_directory, exist_ok=True)\nos.makedirs(settings.upload_directory, exist_ok=True)\n",
  "app/core/config/__init__.py": "\"\"\"\nCore Config Module\n\"\"\"\n\nfrom .settings import Settings\n\nsettings = Settings()\n\n__all__ = [\"settings\"]\n",
  "app/core/database/database.py": "\"\"\"\nDatabase initialization and session management\n\"\"\"\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom app.core.config import settings\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n# Create engine\nengine = create_engine(\n    settings.database_url,\n    connect_args={\"check_same_thread\": False} if \"sqlite\" in settings.database_url else {},\n    echo=settings.echo_sql,\n)\n\n# Create session factory\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n\ndef get_db() -> Session:\n    \"\"\"\n    Get database session.\n\n    Yields:\n        Database session\n    \"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\ndef init_db():\n    \"\"\"Initialize database, creating all tables.\"\"\"\n    from app.core.base.entity import Base\n\n    logger.info(\"Initializing database...\")\n    Base.metadata.create_all(bind=engine)\n    logger.info(\"Database initialized\")\n\n\ndef close_db():\n    \"\"\"Close database connections.\"\"\"\n    logger.info(\"Closing database connections...\")\n    engine.dispose()\n    logger.info(\"Database connections closed\")\n",
  "app/core/database/__init__.py": "\"\"\"\nDatabase module initialization\n\"\"\"\n\nfrom .database import get_db, init_db, close_db, engine, SessionLocal\n\n__all__ = [\"get_db\", \"init_db\", \"close_db\", \"engine\", \"SessionLocal\"]\n",
  "app/core/utils/exceptions.py": "\"\"\"\nCustom exceptions for the application\n\"\"\"\n\n\nclass AppException(Exception):\n    \"\"\"Base application exception.\"\"\"\n\n    def __init__(self, message: str, status_code: int = 500):\n        self.message = message\n        self.status_code = status_code\n        super().__init__(self.message)\n\n\nclass ValidationException(AppException):\n    \"\"\"Exception raised for validation errors.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message, status_code=422)\n\n\nclass DatabaseException(AppException):\n    \"\"\"Exception raised for database errors.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message, status_code=500)\n\n\nclass LLMException(AppException):\n    \"\"\"Exception raised for LLM errors.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message, status_code=500)\n\n\nclass AgentException(AppException):\n    \"\"\"Exception raised for agent errors.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message, status_code=500)\n\n\nclass NotFoundException(AppException):\n    \"\"\"Exception raised when resource is not found.\"\"\"\n\n    def __init__(self, message: str):\n        super().__init__(message, status_code=404)\n",
  "app/core/utils/logger.py": "\"\"\"\nLogging configuration\n\"\"\"\n\nimport logging\nimport sys\nimport os\nfrom datetime import datetime\nfrom typing import Optional\nfrom app.core.config import settings\n\n\ndef get_logger(name: str, level: Optional[str] = None) -> logging.Logger:\n    \"\"\"\n    Get a configured logger instance with file and console handlers.\n\n    Args:\n        name: Logger name (usually __name__)\n        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    logger = logging.getLogger(name)\n\n    if not logger.handlers:\n        # Create logs directory if it doesn't exist\n        os.makedirs(settings.log_directory, exist_ok=True)\n\n        # Console handler\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\"\n        )\n        console_handler.setFormatter(console_formatter)\n        logger.addHandler(console_handler)\n\n        # File handler - separate log files for different levels\n        log_filename = os.path.join(\n            settings.log_directory,\n            f\"app_{datetime.now().strftime('%Y%m%d')}.log\"\n        )\n        file_handler = logging.FileHandler(log_filename)\n        file_formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\"\n        )\n        file_handler.setFormatter(file_formatter)\n        logger.addHandler(file_handler)\n\n        # Error log file\n        error_log_filename = os.path.join(\n            settings.log_directory,\n            f\"error_{datetime.now().strftime('%Y%m%d')}.log\"\n        )\n        error_handler = logging.FileHandler(error_log_filename)\n        error_handler.setLevel(logging.ERROR)\n        error_handler.setFormatter(file_formatter)\n        logger.addHandler(error_handler)\n\n    if level:\n        logger.setLevel(getattr(logging, level.upper(), logging.INFO))\n    else:\n        logger.setLevel(getattr(logging, settings.log_level, logging.INFO))\n\n    return logger\n",
  "app/core/utils/observability.py": "\"\"\"\nObservability utilities for LLM tracing and monitoring.\n\nThis module provides a structural approach to instrument LLM calls across the application.\nUses OpenTelemetry and Phoenix for local, privacy-preserving observability.\n\"\"\"\n\nfrom functools import wraps\nfrom typing import Optional, Dict, Any, Callable\nfrom contextlib import contextmanager\nimport re\nimport time\n\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\nfrom app.core.config import settings\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n# Get tracer instance\ntracer = trace.get_tracer(__name__)\n\n\nclass ObservabilityManager:\n    \"\"\"\n    Centralized manager for observability features.\n    Provides a structural way to add tracing to any LLM-related function.\n    \"\"\"\n    \n    def __init__(self):\n        self.enabled = settings.enable_observability\n        self.pii_redaction_enabled = settings.enable_pii_redaction\n        \n    def is_enabled(self) -> bool:\n        \"\"\"Check if observability is enabled.\"\"\"\n        return self.enabled\n    \n    @staticmethod\n    def redact_pii(text: str) -> str:\n        \"\"\"\n        Redact PII from text (emails, phone numbers, etc.).\n        \n        Args:\n            text: Text that may contain PII\n            \n        Returns:\n            Text with PII redacted\n        \"\"\"\n        if not settings.enable_pii_redaction:\n            return text\n            \n        # Redact email addresses\n        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL_REDACTED]', text)\n        \n        # Redact phone numbers (various formats)\n        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE_REDACTED]', text)\n        text = re.sub(r'\\b\\+\\d{1,3}[-.]?\\d{3,4}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b', '[PHONE_REDACTED]', text)\n        \n        # Redact credit card numbers\n        text = re.sub(r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b', '[CARD_REDACTED]', text)\n        \n        # Redact SSN\n        text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[SSN_REDACTED]', text)\n        \n        return text\n    \n    @staticmethod\n    def calculate_cost(input_tokens: int, output_tokens: int, model: str) -> float:\n        \"\"\"\n        Calculate estimated cost for LLM call.\n        \n        Args:\n            input_tokens: Number of input tokens\n            output_tokens: Number of output tokens\n            model: Model name\n            \n        Returns:\n            Estimated cost in USD\n        \"\"\"\n        # Pricing per 1M tokens (approximate, update as needed)\n        pricing = {\n            \"gemini-2.5-flash\": {\"input\": 0.075, \"output\": 0.30},  # per 1M tokens\n            \"gpt-4\": {\"input\": 30.0, \"output\": 60.0},\n            \"gpt-3.5-turbo\": {\"input\": 0.5, \"output\": 1.5},\n            \"deepseek-r1\": {\"input\": 0.55, \"output\": 2.19},\n        }\n        \n        # Default pricing if model not found\n        default_pricing = {\"input\": 1.0, \"output\": 2.0}\n        \n        model_pricing = pricing.get(model, default_pricing)\n        \n        input_cost = (input_tokens / 1_000_000) * model_pricing[\"input\"]\n        output_cost = (output_tokens / 1_000_000) * model_pricing[\"output\"]\n        \n        return input_cost + output_cost\n    \n    @staticmethod\n    def extract_token_usage(response: Any) -> Dict[str, int]:\n        \"\"\"\n        Extract token usage from LLM response.\n        \n        Args:\n            response: LLM response object\n            \n        Returns:\n            Dictionary with input_tokens, output_tokens, total_tokens\n        \"\"\"\n        tokens = {\n            \"input_tokens\": 0,\n            \"output_tokens\": 0,\n            \"total_tokens\": 0\n        }\n        \n        try:\n            # For LangChain responses\n            if hasattr(response, 'response_metadata'):\n                metadata = response.response_metadata\n                if 'token_usage' in metadata:\n                    usage = metadata['token_usage']\n                    tokens[\"input_tokens\"] = usage.get('prompt_tokens', 0)\n                    tokens[\"output_tokens\"] = usage.get('completion_tokens', 0)\n                    tokens[\"total_tokens\"] = usage.get('total_tokens', 0)\n                elif 'usage_metadata' in metadata:\n                    # Gemini format\n                    usage = metadata['usage_metadata']\n                    tokens[\"input_tokens\"] = usage.get('prompt_token_count', 0)\n                    tokens[\"output_tokens\"] = usage.get('candidates_token_count', 0)\n                    tokens[\"total_tokens\"] = tokens[\"input_tokens\"] + tokens[\"output_tokens\"]\n            \n            # For OpenAI responses\n            elif hasattr(response, 'usage'):\n                tokens[\"input_tokens\"] = response.usage.prompt_tokens\n                tokens[\"output_tokens\"] = response.usage.completion_tokens\n                tokens[\"total_tokens\"] = response.usage.total_tokens\n                \n        except Exception as e:\n            logger.warning(f\"Could not extract token usage: {str(e)}\")\n        \n        return tokens\n\n\n# Global observability manager instance\nobs_manager = ObservabilityManager()\n\n\n@contextmanager\ndef trace_llm_operation(\n    operation_name: str,\n    attributes: Optional[Dict[str, Any]] = None,\n    capture_input: bool = True,\n    capture_output: bool = True\n):\n    \"\"\"\n    Context manager for tracing LLM operations.\n    \n    Structural approach: Use this to wrap any LLM-related operation.\n    \n    Args:\n        operation_name: Name of the operation (e.g., \"llm.chat.invoke\")\n        attributes: Additional attributes to add to the span\n        capture_input: Whether to capture input in span\n        capture_output: Whether to capture output in span\n        \n    Usage:\n        with trace_llm_operation(\"llm.chat.invoke\", {\"model\": \"gpt-4\"}):\n            response = llm.invoke(query)\n    \"\"\"\n    if not obs_manager.is_enabled():\n        yield None\n        return\n    \n    with tracer.start_as_current_span(operation_name) as span:\n        start_time = time.time()\n        \n        # Set initial attributes\n        if attributes:\n            for key, value in attributes.items():\n                # Redact PII if enabled\n                if isinstance(value, str) and obs_manager.pii_redaction_enabled:\n                    value = obs_manager.redact_pii(value)\n                span.set_attribute(key, value)\n        \n        try:\n            yield span\n            span.set_status(Status(StatusCode.OK))\n        except Exception as e:\n            span.set_status(Status(StatusCode.ERROR, str(e)))\n            span.record_exception(e)\n            raise\n        finally:\n            # Record duration\n            duration_ms = (time.time() - start_time) * 1000\n            span.set_attribute(\"duration_ms\", duration_ms)\n\n\ndef trace_llm_call(\n    operation_name: Optional[str] = None,\n    capture_args: bool = False,\n    capture_result: bool = True\n):\n    \"\"\"\n    Decorator for tracing LLM function calls.\n    \n    Structural approach: Apply this decorator to any function that calls an LLM.\n    \n    Args:\n        operation_name: Custom operation name (defaults to function name)\n        capture_args: Whether to capture function arguments\n        capture_result: Whether to capture function result\n        \n    Usage:\n        @trace_llm_call(operation_name=\"custom.llm.call\")\n        def my_llm_function(query: str):\n            return llm.invoke(query)\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def sync_wrapper(*args, **kwargs):\n            if not obs_manager.is_enabled():\n                return func(*args, **kwargs)\n            \n            op_name = operation_name or f\"llm.{func.__name__}\"\n            \n            with tracer.start_as_current_span(op_name) as span:\n                start_time = time.time()\n                \n                # Capture function metadata\n                span.set_attribute(\"function.name\", func.__name__)\n                span.set_attribute(\"function.module\", func.__module__)\n                \n                # Capture arguments if requested\n                if capture_args and kwargs:\n                    for key, value in kwargs.items():\n                        if isinstance(value, (str, int, float, bool)):\n                            attr_value = value\n                            if isinstance(value, str) and obs_manager.pii_redaction_enabled:\n                                attr_value = obs_manager.redact_pii(value)\n                            span.set_attribute(f\"arg.{key}\", attr_value)\n                \n                try:\n                    result = func(*args, **kwargs)\n                    \n                    # Extract and record token usage\n                    tokens = obs_manager.extract_token_usage(result)\n                    if tokens[\"total_tokens\"] > 0:\n                        span.set_attribute(\"llm.input_tokens\", tokens[\"input_tokens\"])\n                        span.set_attribute(\"llm.output_tokens\", tokens[\"output_tokens\"])\n                        span.set_attribute(\"llm.total_tokens\", tokens[\"total_tokens\"])\n                        \n                        # Calculate cost if model info available\n                        if \"model\" in kwargs:\n                            cost = obs_manager.calculate_cost(\n                                tokens[\"input_tokens\"],\n                                tokens[\"output_tokens\"],\n                                kwargs[\"model\"]\n                            )\n                            span.set_attribute(\"llm.estimated_cost_usd\", cost)\n                    \n                    span.set_status(Status(StatusCode.OK))\n                    return result\n                    \n                except Exception as e:\n                    span.set_status(Status(StatusCode.ERROR, str(e)))\n                    span.record_exception(e)\n                    raise\n                finally:\n                    duration_ms = (time.time() - start_time) * 1000\n                    span.set_attribute(\"duration_ms\", duration_ms)\n        \n        @wraps(func)\n        async def async_wrapper(*args, **kwargs):\n            if not obs_manager.is_enabled():\n                return await func(*args, **kwargs)\n            \n            op_name = operation_name or f\"llm.{func.__name__}\"\n            \n            with tracer.start_as_current_span(op_name) as span:\n                start_time = time.time()\n                \n                # Capture function metadata\n                span.set_attribute(\"function.name\", func.__name__)\n                span.set_attribute(\"function.module\", func.__module__)\n                \n                # Capture arguments if requested\n                if capture_args and kwargs:\n                    for key, value in kwargs.items():\n                        if isinstance(value, (str, int, float, bool)):\n                            attr_value = value\n                            if isinstance(value, str) and obs_manager.pii_redaction_enabled:\n                                attr_value = obs_manager.redact_pii(value)\n                            span.set_attribute(f\"arg.{key}\", attr_value)\n                \n                try:\n                    result = await func(*args, **kwargs)\n                    \n                    # Extract and record token usage\n                    tokens = obs_manager.extract_token_usage(result)\n                    if tokens[\"total_tokens\"] > 0:\n                        span.set_attribute(\"llm.input_tokens\", tokens[\"input_tokens\"])\n                        span.set_attribute(\"llm.output_tokens\", tokens[\"output_tokens\"])\n                        span.set_attribute(\"llm.total_tokens\", tokens[\"total_tokens\"])\n                    \n                    span.set_status(Status(StatusCode.OK))\n                    return result\n                    \n                except Exception as e:\n                    span.set_status(Status(StatusCode.ERROR, str(e)))\n                    span.record_exception(e)\n                    raise\n                finally:\n                    duration_ms = (time.time() - start_time) * 1000\n                    span.set_attribute(\"duration_ms\", duration_ms)\n        \n        # Return appropriate wrapper based on function type\n        import asyncio\n        if asyncio.iscoroutinefunction(func):\n            return async_wrapper\n        else:\n            return sync_wrapper\n    \n    return decorator\n\n\ndef add_span_attributes(attributes: Dict[str, Any]):\n    \"\"\"\n    Add attributes to the current active span.\n    \n    Args:\n        attributes: Dictionary of attributes to add\n    \"\"\"\n    if not obs_manager.is_enabled():\n        return\n    \n    span = trace.get_current_span()\n    if span and span.is_recording():\n        for key, value in attributes.items():\n            if isinstance(value, str) and obs_manager.pii_redaction_enabled:\n                value = obs_manager.redact_pii(value)\n            span.set_attribute(key, value)\n\n\ndef record_llm_metrics(\n    model: str,\n    input_tokens: int,\n    output_tokens: int,\n    duration_ms: float,\n    status: str = \"success\"\n):\n    \"\"\"\n    Record LLM metrics for the current span.\n    \n    Args:\n        model: Model name\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        duration_ms: Duration in milliseconds\n        status: Status of the call (success/error)\n    \"\"\"\n    if not obs_manager.is_enabled():\n        return\n    \n    attributes = {\n        \"llm.model\": model,\n        \"llm.input_tokens\": input_tokens,\n        \"llm.output_tokens\": output_tokens,\n        \"llm.total_tokens\": input_tokens + output_tokens,\n        \"llm.duration_ms\": duration_ms,\n        \"llm.status\": status,\n    }\n    \n    # Calculate cost\n    cost = obs_manager.calculate_cost(input_tokens, output_tokens, model)\n    attributes[\"llm.estimated_cost_usd\"] = cost\n    \n    add_span_attributes(attributes)\n",
  "app/core/utils/__init__.py": "\"\"\"\nUtilities module\n\"\"\"\n\nfrom .logger import get_logger\nfrom .exceptions import (\n    AppException,\n    ValidationException,\n    DatabaseException,\n    LLMException,\n    AgentException,\n    NotFoundException,\n)\nfrom .observability import (\n    trace_llm_operation,\n    trace_llm_call,\n    add_span_attributes,\n    record_llm_metrics,\n    obs_manager,\n)\n\n__all__ = [\n    \"get_logger\",\n    \"AppException\",\n    \"ValidationException\",\n    \"DatabaseException\",\n    \"LLMException\",\n    \"AgentException\",\n    \"NotFoundException\",\n    \"trace_llm_operation\",\n    \"trace_llm_call\",\n    \"add_span_attributes\",\n    \"record_llm_metrics\",\n    \"obs_manager\",\n]\n",
  "app/features/__init__.py": "\"\"\"\nFeatures Module - All application features (vertical modules)\n\"\"\"\n\nfrom .auth import router as auth_router\nfrom .users.users_route import router as users_router\nfrom .chat import chat_router\nfrom .documents import documents_router\n\n__all__ = [\n    \"auth_router\",\n    \"users_router\",\n    \"chat_router\",\n    \"documents_router\",\n]\n\n\n",
  "app/features/auth/auth_route.py": "\"\"\"\nAuthentication API endpoints - login and registration only\n\"\"\"\n\nfrom fastapi import APIRouter, status, Request\nfrom fastapi.responses import JSONResponse\nfrom app.features.auth.auth_schemas import UserCreate, UserLogin, UserResponse, TokenResponse\nfrom app.features.users.user_repository import UserRepository\nfrom app.features.auth.jwt import create_access_token\nfrom app.features.auth.auth_utils import hash_password, verify_password\nfrom app.core.utils import get_logger, ValidationException\n\nlogger = get_logger(__name__)\n\nrouter = APIRouter(prefix=\"/auth\", tags=[\"auth\"])\n\n\n@router.post(\"/register\")\ndef register(request: Request, user_data: UserCreate):\n    \"\"\"Register a new user.\"\"\"\n    try:\n        logger.info(f\"Registering new user: {user_data.username}\")\n        repo = UserRepository()\n        \n        # Check if user already exists\n        if repo.get_by_username(user_data.username):\n            logger.warning(f\"Registration failed: username exists: {user_data.username}\")\n            raise ValidationException(\"Username already exists\")\n\n        # allow admin to set role via payload\n        user = repo.create_user(\n            username=user_data.username,\n            password_hash=hash_password(user_data.password),\n            role=user_data.role or \"user\"\n        )\n        user_response = UserResponse.model_validate(user)\n        \n        # Create token for the new user\n\n        \n        return JSONResponse(\n            status_code=status.HTTP_201_CREATED,\n            content=user_response.model_dump(mode=\"json\"),\n        )\n    except ValidationException as e:\n        logger.warning(f\"Registration failed: {str(e)}\")\n        return JSONResponse(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            content={\"detail\": str(e)}\n        )\n    except Exception as e:\n        logger.error(f\"Registration error: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            content={\"detail\": \"Registration failed\"}\n        )\n\n\n@router.post(\"/login\")\ndef login(login_data: UserLogin):\n    \"\"\"Login user and return access token.\"\"\"\n    try:\n        logger.info(f\"Login attempt: {login_data.username}\")\n        repo = UserRepository()\n\n        # Authenticate user\n        user = repo.get_by_username(login_data.username)\n        if not user:\n            logger.warning(f\"Authentication failed: user not found: {login_data.username}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Invalid username or password\"}\n            )\n\n        if not user.is_active:\n            logger.warning(f\"Authentication failed: user inactive: {login_data.username}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Invalid username or password\"}\n            )\n\n        if not verify_password(login_data.password, user.password_hash):\n            logger.warning(f\"Authentication failed: invalid password: {login_data.username}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Invalid username or password\"}\n            )\n\n        # Update last login\n        repo.update_last_login(user.id)\n        logger.info(f\"User authenticated: {login_data.username}\")\n\n\n        # Create token\n        access_token = create_access_token(data={\"sub\": user.id,\"username\": user.username,\"role\": user.role})\n\n        logger.info(f\"User logged in: {user.username}\")\n        user_response = UserResponse.model_validate(user)\n        token_response = TokenResponse(\n            access_token=access_token,\n            token_type=\"bearer\",\n            user=user_response\n        )\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=token_response.model_dump(mode=\"json\"),\n            headers={\"X-Access-Token\": access_token}\n\n        )\n\n    except Exception as e:\n        logger.error(f\"Login error: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Login failed\"}\n        )\n",
  "app/features/auth/auth_schemas.py": "\"\"\"\nUser schemas for validation\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom datetime import datetime\n\n\nclass UserBase(BaseModel):\n    \"\"\"Base user schema.\"\"\"\n\n    username: str = Field(..., min_length=3, max_length=255)\n\n\nclass UserCreate(UserBase):\n    \"\"\"Schema for creating a user.\"\"\"\n\n    password: str = Field(..., min_length=8)\n    role: Optional[str] = \"user\"\n\n\nclass UserLogin(BaseModel):\n    \"\"\"Schema for user login.\"\"\"\n\n    username: str = Field(..., min_length=3)\n    password: str = Field(...)\n\n\nclass UserUpdate(BaseModel):\n    \"\"\"Schema for updating user.\"\"\"\n\n    password: Optional[str] = Field(None, min_length=8)\n\n\nclass UserResponse(UserBase):\n    \"\"\"Schema for user response.\"\"\"\n\n    id: int\n    is_active: bool\n    role: str\n    last_login: Optional[datetime] = None\n    created_at: datetime\n    updated_at: datetime\n\n    class Config:\n        from_attributes = True\n\n\nclass TokenResponse(BaseModel):\n    \"\"\"Schema for token response.\"\"\"\n\n    access_token: str\n    token_type: str = \"bearer\"\n    user: UserResponse\n",
  "app/features/auth/auth_utils.py": "\"\"\"\nAuthentication utilities\n\"\"\"\n\nimport bcrypt\nfrom fastapi import HTTPException, status, Request\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef get_current_user(request: Request):\n    \"\"\"Get the current user from the request state.\"\"\"\n    user = getattr(request.state, \"user\", None)\n    if not user:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\")\n    return user\n\n\ndef hash_password(password: str) -> str:\n    \"\"\"\n    Hash a password using bcrypt.\n    \n    Args:\n        password: Plain text password\n        \n    Returns:\n        Hashed password\n    \"\"\"\n    salt = bcrypt.gensalt()\n    hashed = bcrypt.hashpw(password.encode('utf-8'), salt)\n    return hashed.decode('utf-8')\n\n\ndef verify_password(plain_password: str, hashed_password: str) -> bool:\n    \"\"\"\n    Verify a password against its hash.\n    \n    Args:\n        plain_password: Plain text password to verify\n        hashed_password: Hashed password to check against\n        \n    Returns:\n        True if password matches, False otherwise\n    \"\"\"\n    return bcrypt.checkpw(plain_password.encode('utf-8'), hashed_password.encode('utf-8'))\n",
  "app/features/auth/jwt.py": "\"\"\"\nJWT and authentication utilities\n\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport jwt\nfrom app.core.config import settings\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:\n    \"\"\"\n    Create JWT access token.\n\n    Args:\n        data: Data to encode in token\n        expires_delta: Token expiration time\n\n    Returns:\n        Encoded JWT token\n    \"\"\"\n    to_encode = data.copy()\n    \n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=settings.access_token_expire_minutes)\n\n    to_encode.update({\"exp\": expire})\n\n    encoded_jwt = jwt.encode(\n        to_encode,\n        settings.secret_key,\n        algorithm=settings.algorithm\n    )\n    \n    logger.info(f\"Created access token for user: {data.get('sub')}\")\n    return encoded_jwt\n\n\ndef verify_token(token: str) -> Optional[dict]:\n    \"\"\"\n    Verify and decode JWT token.\n\n    Args:\n        token: JWT token to verify\n\n    Returns:\n        Decoded token data or None if invalid\n    \"\"\"\n    try:\n        payload = jwt.decode(\n            token,\n            settings.secret_key,\n            algorithms=[settings.algorithm]\n        )\n        return payload\n    except jwt.ExpiredSignatureError:\n        logger.warning(\"Token expired\")\n        return None\n    except jwt.InvalidTokenError as e:\n        logger.warning(f\"Invalid token: {str(e)}\")\n        return None\n\n\ndef get_user_id_from_token(token: str) -> Optional[int]:\n    \"\"\"\n    Extract user ID from token.\n\n    Args:\n        token: JWT token\n\n    Returns:\n        User ID or None\n    \"\"\"\n    payload = verify_token(token)\n    if payload:\n        return payload.get(\"sub\")\n    return None\n",
  "app/features/auth/__init__.py": "\"\"\"\nAuth Feature - Authentication (JWT, login, register, password utilities)\n\"\"\"\n\nfrom .jwt import create_access_token, verify_token, get_user_id_from_token\nfrom .auth_route import router\n\n__all__ = [\"router\", \"create_access_token\", \"verify_token\", \"get_user_id_from_token\"]\n",
  "app/features/chat/chat_entity.py": "\"\"\"\nChat entity model\n\"\"\"\n\nfrom sqlalchemy import Column, Integer, String, Text, DateTime, JSON\nfrom app.core.base import BaseEntity\nfrom datetime import datetime\n\nclass ChatMessage(BaseEntity):\n    \"\"\"\n    Chat message entity.\n    Stores all messages for all chats.\n    chat_id groups messages into conversations.\n    \"\"\"\n    __tablename__ = \"chat_messages\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    chat_id = Column(Integer, index=True, nullable=False)\n    user_id = Column(Integer, nullable=False, index=True)  # No ForeignKey, just storing the ID\n    message_type = Column(String(50), nullable=False)  # user, bot, system, error\n    content = Column(Text, nullable=False)\n    metadata_info = Column(JSON, nullable=True)\n\n    def __repr__(self):\n        return f\"<ChatMessage(id={self.id}, chat_id={self.chat_id}, type={self.message_type})>\"\n",
  "app/features/chat/chat_repository.py": "\"\"\"\nChat repository implementation\n\"\"\"\n\nfrom typing import List, Optional, Dict, Any\nfrom sqlalchemy import func, distinct, desc\nfrom app.core.base import BaseRepository\nfrom app.features.chat.chat_entity import ChatMessage\nfrom app.core.utils import get_logger, NotFoundException\nfrom app.llm_functions.LLMCall import CallAgentGraph\n\nlogger = get_logger(__name__)\n\nclass ChatRepository(BaseRepository[ChatMessage]):\n    \"\"\"Repository for ChatMessage entity with bot logic.\"\"\"\n\n    def __init__(self):\n        super().__init__(ChatMessage)\n\n    def create_new_chat_id(self, user_id: int) -> int:\n        \"\"\"Generate a new chat_id for the user.\"\"\"\n        db = self._get_db()\n        # Find the max chat_id across the system or per user? \n        # Usually chat_ids should be unique across system if we want simple /chat/{id}\n        # Let's make them unique across system for simplicity\n        max_id = db.query(func.max(ChatMessage.chat_id)).scalar()\n        return (max_id or 0) + 1\n\n    def verify_chat_ownership(self, chat_id: int, user_id: int) -> bool:\n        \"\"\"Check if the chat belongs to the user.\"\"\"\n        db = self._get_db()\n        # Check if there are any messages with this chat_id and user_id\n        # If no messages exist for this chat_id at all, it's valid (new chat potentially)\n        # But if messages exist, they must match user_id\n        \n        first_msg = db.query(ChatMessage).filter(ChatMessage.chat_id == chat_id).first()\n        if not first_msg:\n            return True # Chat doesn't exist yet, so ownership is fine (will be created)\n            \n        return first_msg.user_id == user_id\n\n    def save_message(self, chat_id: int, user_id: int, message_type: str, content: str, metadata_info: Dict = None) -> ChatMessage:\n        \"\"\"Save a message to the database.\"\"\"\n        db = self._get_db()\n        message = ChatMessage(\n            chat_id=chat_id,\n            user_id=user_id,\n            message_type=message_type,\n            content=content,\n            metadata_info=metadata_info or {}\n        )\n        db.add(message)\n        db.commit()\n        db.refresh(message)\n        return message\n\n    def get_chat_messages(self, chat_id: int, user_id: int, limit: int = 100) -> List[ChatMessage]:\n        \"\"\"Get messages for a specific chat.\"\"\"\n        if not self.verify_chat_ownership(chat_id, user_id):\n            raise NotFoundException(f\"Chat {chat_id} not found or access denied\")\n            \n        db = self._get_db()\n        return db.query(ChatMessage)\\\n            .filter(ChatMessage.chat_id == chat_id)\\\n            .order_by(ChatMessage.created_at.asc())\\\n            .limit(limit)\\\n            .all()\n\n    def get_user_chats(self, user_id: int) -> List[Dict[str, Any]]:\n        \"\"\"Get all chat sessions for a user.\"\"\"\n        db = self._get_db()\n        \n        # Get distinct chat_ids for the user\n        # This is a bit complex with SQLAlchemy to get the latest message for each chat\n        # Simplified: Get distinct chat_ids and their first message creation time\n        \n        subquery = db.query(\n            ChatMessage.chat_id,\n            func.max(ChatMessage.created_at).label('last_update')\n        ).filter(ChatMessage.user_id == user_id)\\\n         .group_by(ChatMessage.chat_id)\\\n         .subquery()\n         \n        results = db.query(subquery.c.chat_id, subquery.c.last_update)\\\n            .order_by(subquery.c.last_update.desc())\\\n            .all()\n            \n        chats = []\n        for r in results:\n            # Get the first message to use as title/preview\n            first_msg = db.query(ChatMessage)\\\n                .filter(ChatMessage.chat_id == r.chat_id)\\\n                .order_by(ChatMessage.created_at.asc())\\\n                .first()\n                \n            chats.append({\n                \"chat_id\": r.chat_id,\n                \"last_update\": r.last_update,\n                \"preview\": first_msg.content[:50] + \"...\" if first_msg else \"Empty chat\"\n            })\n            \n        return chats\n\n    def delete_chat(self, chat_id: int, user_id: int):\n        \"\"\"Delete all messages in a chat.\"\"\"\n        if not self.verify_chat_ownership(chat_id, user_id):\n            raise NotFoundException(f\"Chat {chat_id} not found or access denied\")\n            \n        db = self._get_db()\n        db.query(ChatMessage).filter(ChatMessage.chat_id == chat_id).delete()\n        db.commit()\n\n    async def process_user_message(self, chat_id: int, user_id: int, content: str) -> ChatMessage:\n        \"\"\"\n        Process a user message:\n        1. Save user message\n        2. Retrieve chat history\n        3. Convert to LangChain messages\n        4. Call LLM with history and chat_id\n        5. Save and return bot response\n        \"\"\"\n        from app.core.utils import trace_llm_operation, add_span_attributes\n        from langchain_core.messages import HumanMessage, AIMessage\n        \n        with trace_llm_operation(\n            \"chat.process_message\",\n            attributes={\n                \"chat.id\": chat_id,\n                \"chat.user_id\": user_id,\n                \"chat.message_length\": len(content)\n            }\n        ):\n            # 1. Save user message\n            self.save_message(chat_id, user_id, \"user\", content)\n            \n            add_span_attributes({\n                \"chat.step\": \"user_message_saved\"\n            })\n            \n            try:\n                # 2. Retrieve recent chat history (last 20 messages for context)\n                db = self._get_db()\n                recent_messages = db.query(ChatMessage)\\\n                    .filter(ChatMessage.chat_id == chat_id)\\\n                    .order_by(ChatMessage.created_at.asc())\\\n                    .limit(20)\\\n                    .all()\n                \n                # 3. Convert DB messages to LangChain message format\n                history = []\n                for msg in recent_messages[:-1]:  # Exclude the just-saved user message\n                    if msg.message_type == \"user\":\n                        history.append(HumanMessage(content=msg.content))\n                    elif msg.message_type == \"bot\":\n                        history.append(AIMessage(content=msg.content))\n                    # Skip error messages in history\n                \n                add_span_attributes({\n                    \"chat.step\": \"history_retrieved\",\n                    \"chat.history_size\": len(history)\n                })\n                \n                # 4. Call LLM (Agent Graph) with chat_id and history\n                bot_response_text = await CallAgentGraph(\n                    query=content,\n                    chat_id=chat_id,\n                    history=history if history else None\n                )\n                \n                add_span_attributes({\n                    \"chat.bot_response_length\": len(bot_response_text),\n                    \"chat.step\": \"llm_response_received\"\n                })\n                \n                # 5. Save bot response\n                bot_message = self.save_message(\n                    chat_id, \n                    user_id, \n                    \"bot\", \n                    bot_response_text\n                )\n                \n                add_span_attributes({\n                    \"chat.step\": \"bot_message_saved\",\n                    \"chat.status\": \"success\"\n                })\n                \n                return bot_message\n                \n            except Exception as e:\n                logger.error(f\"Error in bot processing: {str(e)}\", exc_info=True)\n                \n                add_span_attributes({\n                    \"chat.step\": \"error\",\n                    \"chat.status\": \"error\",\n                    \"chat.error\": str(e)\n                })\n                \n                # Save error message\n                error_msg = self.save_message(\n                    chat_id,\n                    user_id,\n                    \"error\",\n                    f\"I encountered an error: {str(e)}\"\n                )\n                return error_msg\n",
  "app/features/chat/chat_route.py": "\"\"\"\nChat API endpoints - WebSocket and REST\n\"\"\"\n\nfrom fastapi import APIRouter, status, WebSocket, WebSocketDisconnect, Query, Depends\nfrom fastapi.responses import JSONResponse\nfrom typing import List\nfrom app.features.chat.chat_schemas import (\n    WSMessageRequest, \n    WSMessageResponse, \n    ChatSessionPreview, \n    ChatHistoryResponse,\n    ChatMessageResponse\n)\nfrom app.features.chat.chat_repository import ChatRepository\nfrom app.core.utils import get_logger\nimport json\n\nlogger = get_logger(__name__)\n\nrouter = APIRouter(prefix=\"/chat\", tags=[\"chat\"])\n\n# WebSocket Endpoint\n@router.websocket(\"/ws/{user_id}/{chat_id}\")\nasync def chat_websocket(websocket: WebSocket, chat_id: int, user_id: int):\n    \"\"\"\n    WebSocket endpoint for real-time chat.\n    \n    Args:\n        chat_id: 0 for new chat, >0 for existing chat\n        user_id: User ID (passed as query param since WS doesn't support headers easily in all clients)\n    \"\"\"\n    await websocket.accept()\n    repo = ChatRepository()\n    \n    try:\n        # Initialize Chat\n        current_chat_id = chat_id\n        \n        if current_chat_id == 0:\n            # Create new chat\n            current_chat_id = repo.create_new_chat_id(user_id)\n            await websocket.send_text(WSMessageResponse(\n                type=\"chat_created\",\n                content=\"New chat session created\",\n                chat_id=current_chat_id\n            ).model_dump_json())\n            logger.info(f\"New chat created: {current_chat_id} for user {user_id}\")\n        else:\n            # Verify existing chat\n            if not repo.verify_chat_ownership(current_chat_id, user_id):\n                await websocket.send_text(WSMessageResponse(\n                    type=\"error\",\n                    content=\"Chat not found or access denied\",\n                    chat_id=current_chat_id\n                ).model_dump_json())\n                await websocket.close()\n                return\n                \n            await websocket.send_text(WSMessageResponse(\n                type=\"chat_loaded\",\n                content=f\"Joined chat {current_chat_id}\",\n                chat_id=current_chat_id\n            ).model_dump_json())\n            logger.info(f\"User {user_id} joined chat {current_chat_id}\")\n\n        # Message Loop\n        while True:\n            data = await websocket.receive_text()\n            \n            try:\n                # Parse message\n                message_data = json.loads(data)\n                request = WSMessageRequest(**message_data)\n                \n                if request.type == \"ping\":\n                    await websocket.send_text(WSMessageResponse(\n                        type=\"pong\",\n                        content=\"pong\",\n                        chat_id=current_chat_id\n                    ).model_dump_json())\n                    continue\n                \n                # Process User Message\n                # 1. Save & Process\n                bot_message = await repo.process_user_message(\n                    current_chat_id, \n                    user_id, \n                    request.content\n                )\n                \n                # 2. Send Response\n                await websocket.send_text(WSMessageResponse(\n                    type=\"response\",\n                    content=bot_message.content,\n                    chat_id=current_chat_id,\n                    message_id=bot_message.id\n                ).model_dump_json())\n                \n            except json.JSONDecodeError:\n                await websocket.send_text(WSMessageResponse(\n                    type=\"error\",\n                    content=\"Invalid JSON format\",\n                    chat_id=current_chat_id\n                ).model_dump_json())\n            except Exception as e:\n                logger.error(f\"Error processing message: {str(e)}\", exc_info=True)\n                await websocket.send_text(WSMessageResponse(\n                    type=\"error\",\n                    content=f\"Error: {str(e)}\",\n                    chat_id=current_chat_id\n                ).model_dump_json())\n\n    except WebSocketDisconnect:\n        logger.info(f\"WebSocket disconnected for chat {chat_id}\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {str(e)}\", exc_info=True)\n        try:\n            await websocket.close()\n        except:\n            pass\n\n\n# REST Endpoints\n\n@router.get(\"/sessions\", response_model=List[ChatSessionPreview])\ndef get_user_sessions(user_id: int = Query(...)):\n    \"\"\"Get all chat sessions for a user.\"\"\"\n    try:\n        repo = ChatRepository()\n        sessions = repo.get_user_chats(user_id)\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=[\n                ChatSessionPreview(\n                    chat_id=s[\"chat_id\"],\n                    last_update=s[\"last_update\"],\n                    preview=s[\"preview\"]\n                ).model_dump(mode=\"json\") \n                for s in sessions\n            ]\n        )\n    except Exception as e:\n        logger.error(f\"Error fetching sessions: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Error fetching chat sessions\"}\n        )\n\n\n@router.get(\"/{chat_id}/history\", response_model=ChatHistoryResponse)\ndef get_chat_history(chat_id: int, user_id: int = Query(...)):\n    \"\"\"Get full history of a chat session.\"\"\"\n    try:\n        repo = ChatRepository()\n        messages = repo.get_chat_messages(chat_id, user_id)\n        \n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=ChatHistoryResponse(\n                chat_id=chat_id,\n                messages=[ChatMessageResponse.model_validate(m) for m in messages]\n            ).model_dump(mode=\"json\")\n        )\n    except Exception as e:\n        logger.error(f\"Error fetching history: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": str(e)}\n        )\n\n\n@router.delete(\"/{chat_id}\")\ndef delete_chat(chat_id: int, user_id: int = Query(...)):\n    \"\"\"Delete a chat session.\"\"\"\n    try:\n        repo = ChatRepository()\n        repo.delete_chat(chat_id, user_id)\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content={\"detail\": \"Chat deleted successfully\"}\n        )\n    except Exception as e:\n        logger.error(f\"Error deleting chat: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": str(e)}\n        )\n\n\n@router.get(\"/health\")\ndef health_check():\n    \"\"\"Health check endpoint for the chat service\"\"\"\n    logger.info(\"Chat service health check\")\n    return JSONResponse(\n        status_code=status.HTTP_200_OK,\n        content={\"status\": \"ok\", \"service\": \"chat_websocket\"}\n    )\n",
  "app/features/chat/chat_schemas.py": "\"\"\"\nChat schemas for request/response validation\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List, Dict, Any\nfrom datetime import datetime\n\n\n# WebSocket Schemas\nclass WSMessageRequest(BaseModel):\n    \"\"\"Incoming WebSocket message from client.\"\"\"\n    type: str = Field(default=\"message\", description=\"Message type: message, ping\")\n    content: str = Field(..., description=\"Message content\")\n\n\nclass WSMessageResponse(BaseModel):\n    \"\"\"Outgoing WebSocket message to client.\"\"\"\n    type: str = Field(..., description=\"Response type: response, error, chat_created, chat_loaded\")\n    content: str = Field(..., description=\"Message content\")\n    chat_id: int = Field(..., description=\"Chat Session ID\")\n    message_id: Optional[int] = Field(None, description=\"Database Message ID\")\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    \n    class Config:\n        json_encoders = {\n            datetime: lambda v: v.isoformat()\n        }\n\n\n# REST Schemas\nclass ChatMessageResponse(BaseModel):\n    \"\"\"Schema for a single chat message.\"\"\"\n    id: int\n    chat_id: int\n    message_type: str\n    content: str\n    created_at: datetime\n    metadata_info: Optional[Dict[str, Any]] = None\n    \n    class Config:\n        from_attributes = True\n\n\nclass ChatSessionPreview(BaseModel):\n    \"\"\"Schema for chat session list item.\"\"\"\n    chat_id: int\n    last_update: datetime\n    preview: str\n\n\nclass ChatHistoryResponse(BaseModel):\n    \"\"\"Schema for full chat history.\"\"\"\n    chat_id: int\n    messages: List[ChatMessageResponse]\n",
  "app/features/chat/__init__.py": "from app.features.chat.chat_route import router as chat_router\n\n__all__ = [\"chat_router\"]\n",
  "app/features/documents/router.py": "from fastapi import APIRouter, UploadFile, File, HTTPException\nfrom app.features.documents.service import DocumentService\nfrom app.features.documents.schemas import UploadResponse, RetrieveRequest, RetrieveResponse, DocumentChunk\n\nrouter = APIRouter(tags=[\"Documents\"])\nservice = DocumentService()\n\n@router.post(\"/documents/upload/storage\", response_model=UploadResponse)\nasync def upload_to_storage(file: UploadFile = File(...)):\n    \"\"\"\n    Upload a document to storage without embedding.\n    \"\"\"\n    try:\n        file_path = await service.save_file(file)\n        return UploadResponse(\n            filename=file.filename,\n            file_path=file_path,\n            message=\"File uploaded successfully to storage\"\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post(\"/documents/upload/embed\", response_model=UploadResponse)\nasync def upload_and_embed(file: UploadFile = File(...)):\n    \"\"\"\n    Upload a document, parse it, and store its embeddings.\n    \"\"\"\n    try:\n        file_path, chunks_count = await service.process_and_embed(file)\n        return UploadResponse(\n            filename=file.filename,\n            file_path=file_path,\n            message=f\"File uploaded and embedded successfully. Created {chunks_count} chunks.\"\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post(\"/documents/retrieve\", response_model=RetrieveResponse)\nasync def retrieve_documents(request: RetrieveRequest):\n    \"\"\"\n    Retrieve relevant document chunks based on a query.\n    \"\"\"\n    try:\n        results = await service.retrieve(request.query, request.top_k)\n        chunks = [\n            DocumentChunk(content=doc.page_content, metadata=doc.metadata) \n            for doc in results\n        ]\n        return RetrieveResponse(results=chunks)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n",
  "app/features/documents/schemas.py": "from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\n\nclass UploadResponse(BaseModel):\n    filename: str\n    file_path: str\n    message: str\n    document_id: Optional[str] = None\n\nclass RetrieveRequest(BaseModel):\n    query: str\n    top_k: int = 4\n\nclass DocumentChunk(BaseModel):\n    content: str\n    metadata: Dict[str, Any]\n\nclass RetrieveResponse(BaseModel):\n    results: List[DocumentChunk]\n",
  "app/features/documents/service.py": "import os\nimport shutil\nfrom typing import List\nfrom fastapi import UploadFile, HTTPException\nfrom app.core.config import settings\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\nfrom langchain_core.documents import Document\nfrom app.llm_functions.RAGHelper import RAGHelper\n\nclass DocumentService:\n    def __init__(self):\n        self.upload_dir = settings.upload_directory\n        # Ensure upload directory exists\n        os.makedirs(self.upload_dir, exist_ok=True)\n        self.rag_helper = RAGHelper()\n\n    async def save_file(self, file: UploadFile) -> str:\n        file_path = os.path.join(self.upload_dir, file.filename)\n        with open(file_path, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n        return file_path\n\n    async def parse_document(self, file_path: str) -> List[Document]:\n        ext = os.path.splitext(file_path)[1].lower()\n        try:\n            if ext == \".pdf\":\n                loader = PyPDFLoader(file_path)\n            elif ext == \".txt\":\n                loader = TextLoader(file_path)\n            elif ext == \".csv\":\n                loader = CSVLoader(file_path)\n            else:\n                raise HTTPException(status_code=400, detail=f\"Unsupported file type: {ext}\")\n            \n            return loader.load()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Error parsing file: {str(e)}\")\n\n    async def process_and_embed(self, file: UploadFile):\n        # 1. Save file\n        file_path = await self.save_file(file)\n        \n        # 2. Parse\n        documents = await self.parse_document(file_path)\n        \n        # 3. Embed and Store (Delegated to RAGHelper)\n        num_chunks = self.rag_helper.embed_documents(documents)\n        \n        return file_path, num_chunks\n\n    async def retrieve(self, query: str, top_k: int = 4) -> List[Document]:\n        # Delegated to RAGHelper\n        return self.rag_helper.retrieve(query, top_k)\n",
  "app/features/documents/__init__.py": "from .router import router as documents_router\n",
  "app/features/users/test_user_repository.py": "# tests/test_repository.py (or conftest.py)\nimport pytest\nfrom unittest.mock import patch, Mock\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\nfrom app.features.users.user_entity import User\nfrom app.features.users.user_repository import UserRepository\nfrom sqlalchemy.exc import IntegrityError\nfrom app.core.base.entity import Base\n\n# Fixture to create an in-memory SQLite database and tear it down\n@pytest.fixture\ndef db_session():\n    # 1. SETUP: Create an in-memory SQLite engine\n    # 'sqlite:///:memory:' means the DB exists only in RAM for the test duration\n    engine = create_engine(\"sqlite:///:memory:\") \n    \n    # 2. SETUP: Create all tables defined in Base (our Item table)\n    Base.metadata.create_all(engine)\n    \n    # 3. SETUP: Create a session\n    session = Session(engine)\n\n    # 4. YIELD: Pass the session to the test function\n    yield session \n\n    # 5. TEARDOWN: Close and clean up the session/engine\n    session.close()\n    Base.metadata.drop_all(engine)\n    \n\n# ----------------------------------------------------\n# Actual Test Cases\n# ----------------------------------------------------\n\n@patch('app.core.base.repository.SessionLocal') \ndef test_create_and_get_by_username(MockSessionLocal, db_session: Session):\n    MockSessionLocal.return_value = db_session\n    repo = UserRepository() \n\n    # ACT: Use the repository method (which uses the injected test session)\n    new_user = repo.create_user(username=\"TestUser\", password_hash=\"dfdsfsdfsd\",role=\"user\")\n\n    # ASSERT 1: Verify the session was called (i.e., the BaseRepo ran its __init__)\n    MockSessionLocal.assert_called_once()\n    \n    # ASSERT 2: Verify the user was written to the test database\n    retrieved_user = repo.get_by_username(username=\"TestUser\")\n    assert retrieved_user.username == \"TestUser\"\n    assert retrieved_user.role == \"user\"\n    assert retrieved_user.password_hash == \"dfdsfsdfsd\"\n    assert retrieved_user.is_active==True\n\n\n@patch('app.core.base.repository.SessionLocal') \ndef test_get_by_incorrect_username(MockSessionLocal, db_session: Session):\n    \n    MockSessionLocal.return_value = db_session\n    \n    repo = UserRepository() \n\n    # ASSERT 1: Verify using Incorrect User doesnt exist\n    retrieved_item = repo.get_by_username(username=\"TestUser2\")\n\n    MockSessionLocal.assert_called_once()\n    \n    assert retrieved_item is None\n\n\n@patch('app.core.base.repository.SessionLocal') \ndef test_Activate_DeActivate_User(MockSessionLocal, db_session: Session):\n    #\n    MockSessionLocal.return_value = db_session\n    repo = UserRepository() \n\n    # ACT: Use the repository method (which uses the injected test session)\n    new_user = repo.create_user(username=\"TestUser2\", password_hash=\"dfdsfsdfsd\",role=\"user\")\n\n    # ASSERT 1: Verify the session was called (i.e., the BaseRepo ran its __init__)\n    MockSessionLocal.assert_called_once()\n    \n    # ASSERT 2: Verify the user was written to the test database\n    retrieved_user = repo.get_by_username(username=\"TestUser2\")\n    user_id=retrieved_user.id\n    assert retrieved_user.username == \"TestUser2\"\n    assert retrieved_user.role == \"user\"\n    assert retrieved_user.password_hash == \"dfdsfsdfsd\"\n    assert retrieved_user.is_active==True\n\n    #Deactivate the User\n    updated_user = repo.deactivate_user(user_id)\n\n    retrieved_user = repo.get_by_username(username=\"TestUser2\")\n    # ASSERT 3: Verify the user was deactivated\n    assert retrieved_user.is_active==False\n\n    #Activate the User Again\n    updated_user = repo.activate_user(user_id)\n\n    retrieved_user = repo.get_by_username(username=\"TestUser2\")\n    # ASSERT 4: Verify the user is reactivated\n    assert retrieved_user.is_active==True\n\n\n",
  "app/features/users/test_user_route.py": "# tests/test_routers.py\nfrom unittest.mock import patch, Mock\nfrom fastapi.testclient import TestClient\nfrom main import app # Assuming app.main includes the router\nfrom app.features.users.user_repository import UserRepository\n\nclient = TestClient(app)\nAUTH_MIDDLEWARE_DISPATCH_TARGET = 'app.middleware.auth_middleware.AuthMiddleware.dispatch'\n\n\n# --- ASYNCHRONOUS MOCK FUNCTION ---\n# This is the replacement for the original dispatch method.\n# It takes the necessary arguments and simply calls the next handler (the router).\nasync def mock_auth_middleware_dispatch(self, request, call_next):\n    \"\"\"Bypasses authentication logic and immediately passes control to the router.\"\"\"\n    \n    request.state.user = {\"id\": 1, \"username\": \"TestUser\", \"role\": \"user\"}\n    response = await call_next(request)\n    return response\n\n\n\n# ----------------------------------------------------\n# Test Case 1: Get User by Id\n# ----------------------------------------------------\n@patch('app.features.users.users_route.UserRepository') \n@patch(AUTH_MIDDLEWARE_DISPATCH_TARGET, new=mock_auth_middleware_dispatch)\ndef test_get_user(MockUserRepository):\n    # ARRANGE 1: Get the mock instance that represents the actual repository object (repo = ItemRepository())\n    mock_repo_instance = MockUserRepository.return_value \n    \n    expected_db_return = {\n  \"username\": \"JDOE\",\n  \"id\": 3,\n  \"is_active\": True,\n  \"role\": \"user\",\n  \"last_login\": None,\n  \"created_at\": \"2025-12-03T20:28:33.686708\",\n  \"updated_at\": \"2025-12-03T20:28:33.686708\"\n}\n    \n    # ARRANGE 2: Define what the mocked method should return\n    mock_repo_instance.get_by_id.return_value = expected_db_return\n    \n    # ACT: Send a valid request\n    targetId=1\n    response = client.get(\n       f\"api/v1/users/{targetId}\"\n    )\n    \n    # ASSERT 1: Check HTTP response\n    assert response.status_code == 200\n    assert response.json()==expected_db_return\n    # ASSERT 2: Verify the router called the mock repo correctly\n    MockUserRepository.assert_called_once()\n    mock_repo_instance.get_by_id.assert_called_once_with(1)\n\n\n# ----------------------------------------------------\n# Test Case 2: Get All users\n# ----------------------------------------------------\n@patch('app.features.users.users_route.UserRepository') \n@patch(AUTH_MIDDLEWARE_DISPATCH_TARGET, new=mock_auth_middleware_dispatch)\ndef test_get_users(MockUserRepository):\n    # ARRANGE 1: Get the mock instance that represents the actual repository object (repo = ItemRepository())\n    mock_repo_instance = MockUserRepository.return_value \n    \n    expected_db_return = [{\n  \"username\": \"JDOE\",\n  \"id\": 3,\n  \"is_active\": True,\n  \"role\": \"user\",\n  \"last_login\": None,\n  \"created_at\": \"2025-12-03T20:28:33.686708\",\n  \"updated_at\": \"2025-12-03T20:28:33.686708\"\n},\n{\n  \"username\": \"JANEDOE\",\n  \"id\": 4,\n  \"is_active\": True,\n  \"role\": \"user\",\n  \"last_login\": None,\n  \"created_at\": \"2025-12-03T20:28:33.686708\",\n  \"updated_at\": \"2025-12-03T20:28:33.686708\"\n}]\n    \n    # ARRANGE 2: Define what the mocked method should return\n    mock_repo_instance.get_active_users.return_value = expected_db_return\n    \n    # ACT: Send a valid request\n    targetId=1\n    response = client.get(\n       f\"api/v1/users\"\n    )\n    \n    # ASSERT 1: Check HTTP response\n    assert response.status_code == 200\n    assert response.json()==expected_db_return\n    # ASSERT 2: Verify the router called the mock repo correctly\n    MockUserRepository.assert_called_once()\n    mock_repo_instance.get_active_users.assert_called_once_with()\n\n\n\n\n",
  "app/features/users/users_route.py": "\"\"\"\nUser management API endpoints - CRUD operations\n\"\"\"\n\nfrom fastapi import APIRouter, status, Request\nfrom fastapi.responses import JSONResponse\nfrom app.features.users.user_repository import UserRepository\nfrom app.features.auth.auth_schemas import UserResponse, UserUpdate\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\nrouter = APIRouter(prefix=\"/users\", tags=[\"users\"])\n\n@router.get(\"/\")\ndef get_users():\n    \"\"\"Get all active users (requires authentication via middleware).\"\"\"\n    try:\n        repo = UserRepository()\n        users = repo.get_active_users()\n        users_data = [UserResponse.model_validate(user).model_dump(mode=\"json\") for user in users]\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=users_data\n        )\n    except Exception as e:\n        logger.error(f\"Error fetching users: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Error fetching users\"}\n        )\n\n@router.get(\"/{user_id}\")\ndef get_user(user_id: int):\n    \"\"\"Get user by ID (requires authentication via middleware).\"\"\"\n    try:\n        repo = UserRepository()\n        user = repo.get_by_id(user_id)\n        if not user:\n            return JSONResponse(\n                status_code=status.HTTP_404_NOT_FOUND,\n                content={\"detail\": \"User not found\"}\n            )\n        user_response = UserResponse.model_validate(user)\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=user_response.model_dump(mode=\"json\")\n        )\n    except Exception as e:\n        logger.error(f\"Error fetching user: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Error fetching user\"}\n        )\n\n@router.put(\"/{user_id}\")\ndef update_user(user_id: int, user: UserUpdate, request: Request = None):\n    \"\"\"Update user profile (requires authentication via middleware).\"\"\"\n    try:\n        repo = UserRepository()\n        updated_user = repo.update_user(user_id, user)\n        user_response = UserResponse.model_validate(updated_user)\n        return JSONResponse(\n            status_code=status.HTTP_200_OK,\n            content=user_response.model_dump(mode=\"json\")\n        )\n    except Exception as e:\n        logger.error(f\"Error updating user: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Error updating user\"}\n        )\n\n@router.delete(\"/{user_id}\")\ndef delete_user(user_id: int):\n    \"\"\"Delete user account (requires authentication via middleware).\"\"\"\n    try:\n        repo = UserRepository()\n        repo.delete_user(user_id)\n        return JSONResponse(\n            status_code=status.HTTP_204_NO_CONTENT,\n            content={}\n        )\n    except Exception as e:\n        logger.error(f\"Error deleting user: {str(e)}\", exc_info=True)\n        return JSONResponse(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            content={\"detail\": \"Error deleting user\"}\n        )\n",
  "app/features/users/user_entity.py": "\"\"\"\nUser entity model\n\"\"\"\n\nfrom sqlalchemy import Column, Integer, String, Boolean, DateTime\nfrom sqlalchemy.orm import relationship\nfrom app.core.base import BaseEntity\nfrom datetime import datetime\n\n\nclass User(BaseEntity):\n    \"\"\"User entity model.\"\"\"\n\n    __tablename__ = \"users\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    username = Column(String(255), unique=True, nullable=False, index=True)\n    password_hash = Column(String(255), nullable=False)\n    is_active = Column(Boolean, default=True, nullable=False)\n    role = Column(String(50), default=\"user\", nullable=False)\n    last_login = Column(DateTime, nullable=True)\n\n    def __repr__(self):\n        return f\"<User(id={self.id}, username={self.username})>\"\n",
  "app/features/users/user_repository.py": "\"\"\"\nUser repository implementation\n\"\"\"\n\nfrom typing import Optional\nfrom app.core.base import BaseRepository\nfrom app.features.users.user_entity import User\nfrom app.core.utils import get_logger, NotFoundException\n\nlogger = get_logger(__name__)\n\n\nclass UserRepository(BaseRepository[User]):\n    \"\"\"Repository for User entity with custom queries.\"\"\"\n\n    def __init__(self):\n        super().__init__(User)\n\n    def get_by_username(self, username: str) -> Optional[User]:\n        \"\"\"Get user by username.\"\"\"\n        logger.info(f\"Fetching user by username: {username}\")\n        db = self._get_db()\n        return db.query(User).filter(User.username == username).first()\n\n    def get_active_users(self, skip: int = 0, limit: int = 100) -> list[User]:\n        \"\"\"Get all active users.\"\"\"\n        logger.info(f\"Fetching active users (skip={skip}, limit={limit})\")\n        db = self._get_db()\n        return db.query(User).filter(User.is_active == True).offset(skip).limit(limit).all()\n\n    def create_user(self, username: str, password_hash: str, role: str = \"user\") -> User:\n        \"\"\"Create a new user.\"\"\"\n        logger.info(f\"Creating user: {username}\")\n        db = self._get_db()\n        user = User(username=username, password_hash=password_hash, role=role)\n        db.add(user)\n        db.commit()\n        db.refresh(user)\n        logger.info(f\"User created: {username} (id={user.id})\")\n        return user\n\n    def update_user(self, user_id: int, user_data) -> User:\n        \"\"\"Update user profile.\"\"\"\n        user = self.get_by_id(user_id)\n        if not user:\n            raise NotFoundException(\"User not found\")\n        \n        update_data = user_data.model_dump(exclude_unset=True, exclude_none=True)\n        for key, value in update_data.items():\n            if hasattr(user, key) and key != \"id\":\n                setattr(user, key, value)\n        \n        db = self._get_db()\n        db.commit()\n        db.refresh(user)\n        return user\n\n    def delete_user(self, user_id: int):\n        \"\"\"Delete user account.\"\"\"\n        user = self.get_by_id(user_id)\n        if not user:\n            raise NotFoundException(\"User not found\")\n        return self.delete(user_id)\n\n    def deactivate_user(self, user_id: int) -> Optional[User]:\n        \"\"\"Deactivate user account.\"\"\"\n        user = self.get_by_id(user_id)\n        if user:\n            db = self._get_db()\n            user.is_active = False\n            db.commit()\n            db.refresh(user)\n            logger.info(f\"User deactivated: {user_id}\")\n        return user\n\n    def activate_user(self, user_id: int) -> Optional[User]:\n        \"\"\"Activate user account.\"\"\"\n        user = self.get_by_id(user_id)\n        if user:\n            db = self._get_db()\n            user.is_active = True\n            db.commit()\n            db.refresh(user)\n            logger.info(f\"User activated: {user_id}\")\n        return user\n\n    def update_last_login(self, user_id: int):\n        \"\"\"Update user's last login timestamp.\"\"\"\n        from datetime import datetime\n        user = self.get_by_id(user_id)\n        if user:\n            db = self._get_db()\n            user.last_login = datetime.utcnow()\n            db.commit()\n            logger.info(f\"Updated last login for user: {user_id}\")\n",
  "app/features/users/__init__.py": "\"\"\"\nUsers Feature - User management (CRUD operations)\n\"\"\"\n\nfrom app.features.users.users_route import router\n\n__all__ = [\"router\"]\n",
  "app/llm_functions/AgentGraph.py": "\"\"\"\nAgent Graph - LangGraph workflow with two sample agents for testing\n1. Guardrail Agent - Validates and checks input constraints\n2. Synthesize Response Agent - Generates the final response\nIntegrates with MCP tools for extended functionality\n\"\"\"\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing_extensions import Literal\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\nfrom app.llm_functions.AgentState import AgentState\nfrom app.llm_functions.AgentLLM import get_base_llm, get_reasoning_llm\nfrom app.core.utils import get_logger, trace_llm_operation, add_span_attributes\nfrom app.llm_functions.MCPHelper import GetMCPConfig,InvokeLLMWithMCP\nfrom app.llm_functions.ToolHelper import InvokeLLMWithTool\nlogger = get_logger(__name__)\n\n\nasync def guardrail_agent(state: AgentState) -> dict:\n    \"\"\"\n    Guardrail Agent - Validates input and checks basic constraints.\n    This agent ensures the query is valid and safe to process.\n    \"\"\"\n    with trace_llm_operation(\n        \"agent.guardrail\",\n        attributes={\n            \"agent.name\": \"guardrail\",\n            \"agent.type\": \"validation\",\n            \"agent.chat_id\": state.get(\"chat_id\", \"unknown\")\n        }\n    ):\n        chat_id = state.get(\"chat_id\", \"unknown\")\n        logger.info(f\"--- Executing Guardrail Agent (chat_id: {chat_id}) ---\")\n        messages = state[\"messages\"]\n        \n        guardrail_prompt = SystemMessage(\n            content=\"\"\"You are a guardrail agent. Your job is to validate the user's query.\n            Check if the query is:\n            1. Not empty or nonsensical\n            2. Safe to process\n            3. In a reasonable length\n            \n            Respond with ONLY one word: 'pass' if the query is valid and safe.\n            Respond with ONLY one word: 'fail' if the query is invalid, offensive, or unsafe.\"\"\"\n        )\n        \n        latest_query = messages[-1]\n        validation_messages = [guardrail_prompt, latest_query]\n        \n        add_span_attributes({\n            \"agent.query_content\": str(latest_query.content)[:100],  # First 100 chars\n            \"agent.message_count\": len(messages)\n        })\n        \n        validation_result = get_reasoning_llm().invoke(validation_messages).content.strip().lower()\n        logger.info(f\"Guardrail validation result: {validation_result}\")\n        \n        add_span_attributes({\n            \"agent.validation_result\": validation_result,\n            \"agent.status\": \"pass\" if validation_result == \"pass\" else \"fail\"\n        })\n        \n        return {\"guardrail_status\": validation_result}\n\n\nasync def route_guardrail(state: AgentState) -> Literal[\"synthesize\", \"reject\"]:\n    \"\"\"\n    Conditional Edge: Routes based on guardrail validation.\n    \"\"\"\n    status = state.get(\"guardrail_status\", \"fail\")\n    return \"synthesize\" if status == \"pass\" else \"reject\"\n\n\nasync def synthesize_response_agent(state: AgentState) -> dict:\n    \"\"\"\n    Synthesize Response Agent - Generates the final response to the user query.\n    This agent creates a comprehensive and helpful response.\n    May use MCP tools if needed.\n    \"\"\"\n    with trace_llm_operation(\n        \"agent.synthesize\",\n        attributes={\n            \"agent.name\": \"synthesize\",\n            \"agent.type\": \"response_generation\",\n            \"agent.chat_id\": state.get(\"chat_id\", \"unknown\")\n        }\n    ):\n        chat_id = state.get(\"chat_id\", \"unknown\")\n        logger.info(f\"--- Executing Synthesize Response Agent (chat_id: {chat_id}) ---\")\n        messages = state[\"messages\"]\n        mcpConfig=await GetMCPConfig()\n        synthesis_prompt = SystemMessage(\n            content=\"\"\"You are a helpful response synthesis agent. Your job is to generate a clear, \n            concise, and helpful response to the user's query. \n            Keep the response professional and informative.\n            \n            If you need to access current date/time or database information, you can use available tools.\"\"\"\n        )\n        \n        synthesis_messages = [synthesis_prompt] + messages\n        llm = get_base_llm()\n        llmcallinput=  {\"messages\": synthesis_messages}\n        \n        add_span_attributes({\n            \"agent.message_count\": len(synthesis_messages),\n            \"agent.has_mcp_tools\": True\n        })\n        \n        #if using MCP\n        # response =await InvokeLLMWithMCP(llm,llmcallinput,mcpConfig)\n        # if  isinstance(response,list):\n        #     response=response[0]['text']\n\n        response= await InvokeLLMWithTool(llm,messages,['CurrentDate','Search'])\n\n        #response = get_base_llm().invoke(synthesis_messages).content.strip()\n        logger.info(f\"Generated response: {response}\")\n        \n        add_span_attributes({\n            \"agent.response_length\": len(response),\n            \"agent.status\": \"success\"\n        })\n        \n        return {\"messages\": state[\"messages\"] + [SystemMessage(content=response)]}\n\n\nasync def reject_query(state: AgentState) -> dict:\n    \"\"\"\n    Reject invalid or unsafe queries.\n    \"\"\"\n    with trace_llm_operation(\n        \"agent.reject\",\n        attributes={\n            \"agent.name\": \"reject\",\n            \"agent.type\": \"rejection\",\n            \"agent.chat_id\": state.get(\"chat_id\", \"unknown\")\n        }\n    ):\n        chat_id = state.get(\"chat_id\", \"unknown\")\n        logger.info(f\"--- Rejecting invalid query (chat_id: {chat_id}) ---\")\n        reject_message = SystemMessage(\n            content=\"Your query did not pass validation. Please ensure your input is valid and try again.\"\n        )\n        \n        add_span_attributes({\n            \"agent.status\": \"rejected\"\n        })\n        \n        return {\"messages\": state[\"messages\"] + [reject_message]}\n\n# Build the workflow\nworkflow = StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"guardrail_agent\", guardrail_agent)\nworkflow.add_node(\"synthesize_response_agent\", synthesize_response_agent)\nworkflow.add_node(\"reject_query\", reject_query)\n\n# Set start edge\nworkflow.add_edge(START, \"guardrail_agent\")\n\n# Add conditional edges\nworkflow.add_conditional_edges(\n    \"guardrail_agent\",\n    route_guardrail,\n    {\n        \"synthesize\": \"synthesize_response_agent\",\n        \"reject\": \"reject_query\",\n    }\n)\n\n# Add end edges\nworkflow.add_edge(\"synthesize_response_agent\", END)\nworkflow.add_edge(\"reject_query\", END)\n\n# Compile with checkpointer\ncheckpointer = InMemorySaver()\nagentgraph = workflow.compile(checkpointer=checkpointer)\n\n",
  "app/llm_functions/AgentLLM.py": "\"\"\"\nAgent LLM - Provides LLM instances for the agent graph\n\"\"\"\n\nfrom app.llm_functions.LLMDefination import get_chat_llm, ModelCapability\nfrom app.core.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\ndef get_base_llm():\n    \"\"\"\n    Get base LLM for general responses.\n    Uses BASIC capability for simple text generation.\n    \"\"\"\n    logger.info(\"Initializing Base LLM\")\n    return get_chat_llm(capability=ModelCapability.BASIC)\n\n\ndef get_reasoning_llm():\n    \"\"\"\n    Get reasoning LLM for complex logic and validation.\n    Uses REASONING capability for advanced analysis.\n    \"\"\"\n    logger.info(\"Initializing Reasoning LLM\")\n    return get_chat_llm(capability=ModelCapability.REASONING)\n\n\n\n\n\n\n",
  "app/llm_functions/AgentState.py": "\"\"\"\nAgent State - Type definition for LangGraph state\n\"\"\"\n\nfrom typing import Annotated, TypedDict\nfrom langchain_core.messages import AnyMessage\nimport operator\n\n\nclass AgentState(TypedDict):\n    \"\"\"State dictionary for the agent graph, scoped per chat session.\"\"\"\n    chat_id: int  # unique identifier for the chat / LangGraph thread\n    messages: Annotated[list[AnyMessage], operator.add]\n    guardrail_status: str",
  "app/llm_functions/LLMCall.py": "\"\"\"\nLLM Call - Functions to call LLM and Agent Graph\n\"\"\"\n\nfrom typing import List, Optional\nfrom langchain_core.messages import HumanMessage, AnyMessage\nfrom app.llm_functions.LLMDefination import ModelCapability, get_chat_llm\nfrom app.llm_functions.AgentGraph import agentgraph\nfrom app.core.utils import get_logger, trace_llm_call, add_span_attributes\n\nlogger = get_logger(__name__)\n\n\n@trace_llm_call(operation_name=\"llm.direct_call\", capture_args=True)\ndef CallLLM(query: str, capability: ModelCapability = ModelCapability.BASIC, mcp_tools=None):\n    \"\"\"\n    Direct LLM call without agent graph.\n    \n    Args:\n        query: User query string\n        capability: Model capability type\n        mcp_tools: Optional tools for MCP\n        \n    Returns:\n        LLM response\n    \"\"\"\n    logger.info(f\"Calling LLM with query: {query}\")\n    \n    # Add capability to trace\n    add_span_attributes({\n        \"llm.capability\": capability.value,\n        \"llm.query_length\": len(query),\n        \"llm.has_mcp_tools\": mcp_tools is not None\n    })\n    \n    llm = get_chat_llm(capability)\n    response = llm.invoke(query)\n    \n    # Add response metadata\n    add_span_attributes({\n        \"llm.response_length\": len(response.content) if hasattr(response, 'content') else 0\n    })\n    \n    return response\n\n\n@trace_llm_call(operation_name=\"llm.agent_graph\", capture_args=True)\nasync def CallAgentGraph(\n    query: str,\n    chat_id: int,\n    history: Optional[List[AnyMessage]] = None\n):\n    \"\"\"\n    Call agent graph with user query and chat context.\n    Processes query through guardrail and synthesis agents.\n    \n    Args:\n        query: User query string\n        chat_id: Unique chat identifier (used as LangGraph thread_id)\n        history: Optional list of previous messages for context\n        \n    Returns:\n        Final response from the agent graph\n    \"\"\"\n    logger.info(f\"Calling Agent Graph with query: {query}, chat_id: {chat_id}\")\n    \n    # Add query metadata to trace\n    add_span_attributes({\n        \"agent.query_length\": len(query),\n        \"agent.chat_id\": chat_id,\n        \"agent.thread_id\": str(chat_id),\n        \"agent.history_length\": len(history) if history else 0\n    })\n    \n    # Use chat_id as the unique thread identifier for LangGraph\n    config = {\"configurable\": {\"thread_id\": str(chat_id)}}\n    \n    # Build inputs: if history is provided, use it; otherwise start fresh\n    if history:\n        # Append the new query to the existing history\n        inputs = {\n            \"chat_id\": chat_id,\n            \"messages\": history + [HumanMessage(content=query)]\n        }\n    else:\n        # Start a new conversation\n        inputs = {\n            \"chat_id\": chat_id,\n            \"messages\": [HumanMessage(content=query)]\n        }\n    \n    try:\n        response = await agentgraph.ainvoke(inputs, config=config)\n        final_response = response['messages'][-1].content.strip()\n        \n        # Add response metadata\n        add_span_attributes({\n            \"agent.response_length\": len(final_response),\n            \"agent.message_count\": len(response['messages']),\n            \"agent.status\": \"success\"\n        })\n        \n        logger.info(f\"Agent Graph response for chat {chat_id}: {final_response}\")\n        return final_response\n    except Exception as e:\n        add_span_attributes({\n            \"agent.status\": \"error\",\n            \"agent.error\": str(e)\n        })\n        logger.error(f\"Error in Agent Graph for chat {chat_id}: {str(e)}\", exc_info=True)\n        raise\n",
  "app/llm_functions/LLMDefination.py": "from enum import Enum\nimport httpx\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom openai import OpenAI\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom app.core.config import settings\n\n# Global Configuration\n# We use settings for configuration, but keep the HTTP client global for reuse\nHTTP_CLIENT = httpx.Client(verify=False)\n\nclass ModelCapability(Enum):\n    BASIC = \"basic\"           # For simple queries, summaries\n    MODERATE = \"moderate\"\n    REASONING = \"reasoning\"   # For complex logic, math (DeepSeek R1)\n    VISION = \"vision\"         # For image analysis\n    HIGH_PERF = \"high_perf\"   # For code gen, complex nuance\n    EMBEDDING = \"embedding\"   # For RAG/Vector Stores\n    AUDIO = \"audio\"           # For Speech-to-text\n\ndef get_model_name(capability: ModelCapability):\n    \"\"\"Maps capability to the specific model string from settings\"\"\"\n    mapping = {\n        ModelCapability.BASIC: settings.MODEL_CHAT_BASIC,\n        ModelCapability.MODERATE: settings.MODEL_CHAT_MOD,\n        ModelCapability.HIGH_PERF: settings.MODEL_CHAT_OPEN,\n        ModelCapability.REASONING: settings.MODEL_REASONING,\n        ModelCapability.VISION: settings.MODEL_VISION,\n        ModelCapability.EMBEDDING: settings.MODEL_EMBEDDING,\n        ModelCapability.AUDIO: settings.MODEL_AUDIO,\n    }\n    return mapping.get(capability)\n\ndef get_chat_llm(capability: ModelCapability = ModelCapability.BASIC, temperature: float = 0.7):\n    \"\"\"\n    Returns a Chat Model instance (OpenAI or Google) based on configuration.\n    \"\"\"\n    from app.core.utils import trace_llm_operation\n    \n    model_name = get_model_name(capability)\n    \n    # Validation: Ensure we don't pass Embedding/Audio models to the Chat client\n    if capability in [ModelCapability.EMBEDDING, ModelCapability.AUDIO]:\n        raise ValueError(f\"Capability {capability.name} cannot be used with get_chat_llm\")\n\n    # DeepSeek R1 (Reasoning) usually benefits from lower temperature\n    if capability == ModelCapability.REASONING:\n        temperature = 0.1\n    \n    # Trace model initialization\n    with trace_llm_operation(\n        \"llm.model.initialize\",\n        attributes={\n            \"llm.model\": model_name or \"default\",\n            \"llm.capability\": capability.value,\n            \"llm.temperature\": temperature,\n            \"llm.provider\": \"ChatOpenAI\"\n        }\n    ):\n\n            return ChatGoogleGenerativeAI(\n                model=model_name or \"gemini-2.5-flash\",\n                google_api_key=\"\",\n                temperature=temperature,\n                convert_system_message_to_human=True\n            )\n            # Default to OpenAI-compatible (works for OpenAI, DeepSeek, etc.)\n            # return ChatOpenAI(\n            #     base_url=settings.API_ENDPOINT,\n            #     model=model_name or settings.default_model,\n            #     api_key=settings.API_KEY,\n            #     http_client=HTTP_CLIENT,\n            #     temperature=temperature\n            # )\n\ndef get_embeddings():\n    \"\"\"\n    Returns the OpenAIEmbeddings client specifically for Vector operations.\n    \"\"\"\n    model_name = get_model_name(ModelCapability.EMBEDDING)\n    print(f\"Initializing Embeddings: {model_name}\")\n    \n    return OpenAIEmbeddings(\n        base_url=settings.API_ENDPOINT,\n        model=model_name or settings.embedding_model,\n        api_key=settings.API_KEY,\n        http_client=HTTP_CLIENT\n    )\n\ndef get_audio_client():\n    \"\"\"\n    LangChain ChatOpenAI does not support Whisper natively for transcription.\n    We return the raw OpenAI client wrapper for this.\n    \"\"\"\n    print(f\"Initializing Audio Client\")\n    return OpenAI(\n        base_url=settings.API_ENDPOINT,\n        api_key=settings.API_KEY,\n        http_client=HTTP_CLIENT\n    )\n\n",
  "app/llm_functions/MCPHelper.py": "import asyncio\nimport json\nfrom app.core.utils import trace_llm_operation, add_span_attributes\n\nasync def GetMCPConfig():\n    f=open('./app/llm_functions/mcp_config.json')\n    content=f.read()\n    return content\n\nasync def InvokeLLMWithMCP(llm,messages,mcpconfig):\n    \"\"\"Invoke LLM with MCP tools integration.\"\"\"\n    with trace_llm_operation(\n        \"llm.mcp.invoke\",\n        attributes={\n            \"mcp.enabled\": True,\n            \"mcp.config_loaded\": mcpconfig is not None\n        }\n    ):\n        loop = asyncio.get_event_loop()\n        executeTask = loop.create_task(\n            InvokeLLMWithMCPInner(llm,messages,mcpconfig)\n        )\n        response = await executeTask\n        return response\n\ndef GetResponseValue(response):\n    if isinstance(response, str):\n        return response\n    elif hasattr(response, \"content\"):\n        return response.content\n    else:\n        return response\n        \nasync def InvokeLLMWithMCPInner(llm,messages,mcpconfig):\n    \"\"\"Inner function to create and invoke React agent with MCP tools.\"\"\"\n    with trace_llm_operation(\n        \"llm.mcp.react_agent\",\n        attributes={\n            \"agent.type\": \"react\",\n            \"mcp.tools_enabled\": True\n        }\n    ):\n        from langchain_mcp_adapters.client import MultiServerMCPClient\n        from langgraph.prebuilt import create_react_agent\n        mcpconfig=json.loads(mcpconfig)\n        client= MultiServerMCPClient( mcpconfig )\n        tools = await client.get_tools()\n        \n        add_span_attributes({\n            \"mcp.tool_count\": len(tools),\n            \"mcp.servers\": list(mcpconfig.keys()) if isinstance(mcpconfig, dict) else []\n        })\n        \n        agent = create_react_agent(llm,tools,debug=True)\n        resp =await agent.ainvoke(\n               messages\n                )\n        responeMessage=GetResponseValue(resp['messages'][-1])\n        \n        add_span_attributes({\n            \"mcp.response_length\": len(str(responeMessage))\n        })\n        \n        return responeMessage",
  "app/llm_functions/mcp_config.json": "{\n  \"current_date\": {\n    \"command\": \"python\",\n    \"args\": [\n      \"./app/llm_functions/tools/current_date.py\"\n    ],\n    \"transport\": \"stdio\",\n    \"env\": {}\n  },\n  \"google_search\": {\n    \"command\": \"python\",\n    \"args\": [\n      \"./app/llm_functions/tools/google_search.py\"\n    ],\n    \"transport\": \"stdio\",\n    \"env\": {}\n  }\n}",
  "app/llm_functions/RAGHelper.py": "\nimport os\nfrom typing import List\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom app.core.config import settings\nfrom app.llm_functions.LLMDefination import get_embeddings\n\nclass RAGHelper:\n    def __init__(self):\n        self.upload_dir = settings.upload_directory\n        self.vector_store_path = os.path.join(self.upload_dir, \"chroma_db\")\n        # Initialize embeddings using the centralized definition\n        self.embeddings = get_embeddings()\n\n    def embed_documents(self, documents: List[Document]) -> int:\n        \"\"\"\n        Splits and embeds documents into the vector store.\n        Returns the number of chunks created.\n        \"\"\"\n        # Split\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n        splits = text_splitter.split_documents(documents)\n        \n        # Embed and Store\n        # Chroma handles persistence automatically if persist_directory is set\n        Chroma.from_documents(\n            documents=splits, \n            embedding=self.embeddings, \n            persist_directory=self.vector_store_path\n        )\n        return len(splits)\n\n    def retrieve(self, query: str, top_k: int = 4) -> List[Document]:\n        \"\"\"\n        Retrieves relevant documents for a given query.\n        \"\"\"\n        if not os.path.exists(self.vector_store_path):\n             return []\n             \n        vectorstore = Chroma(\n            persist_directory=self.vector_store_path, \n            embedding_function=self.embeddings\n        )\n        return vectorstore.similarity_search(query, k=top_k)\n",
  "app/llm_functions/ToolHelper.py": "\nimport asyncio\nimport json\nfrom app.core.utils import trace_llm_operation, add_span_attributes\nfrom .tools2.toolsconfig import toolsConfig\nfrom langchain.agents import create_agent\n\nasync def InvokeLLMWithTool(llm,messages,toolnames):\n    \"\"\"Invoke LLM with tools integration.\"\"\"\n    with trace_llm_operation(\n        \"llm.mcp.invoke\",\n        attributes={\n            \"mcp.enabled\": True,\n            \"mcp.config_loaded\": toolnames is not None\n        }\n    ):\n        tools=[data['value'] for data in toolsConfig if data['key'] in toolnames]\n        agent= create_agent(llm,tools)\n        response = agent.invoke({\"messages\":messages})\n        finalResponse=response['messages'][-1].content\n        print(finalResponse)\n        if isinstance(finalResponse,list):\n            finalResponse=finalResponse[0]['text']\n\n\n        return finalResponse.strip().lower()\n",
  "app/llm_functions/tools/current_date.py": "\nfrom mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"DateServer\")\nfrom datetime import datetime,UTC\nimport json\n@mcp.tool()\ndef currentDate() -> str:\n    \"\"\"Used to get today's date, current date, current time etc. Should be used to get the date when user asks for current information\n    \"\"\"\n    now = datetime.now(UTC)\n    result = {\n        \"timestamp\": now.isoformat(),\n        \"unix_timestamp\": int(now.timestamp()),\n    }\n    \n    return json.dumps(result)\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n\n\n",
  "app/llm_functions/tools/google_search.py": "\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nimport json\nimport os\nmcp = FastMCP(\"google_search\")\nfrom datetime import datetime\nurl = \"https://google.serper.dev/search\"\n\n\n@mcp.tool()\ndef search(searchstatement:str) -> str:\n    \"\"\"Used to search the web to get information \n        args:\n        searchstatement: The information being searched for in the web\n    \"\"\"\n   \n    if not \"SERPER_API_KEY\" in os.environ:\n         raise Exception(\"Serper API key is not defined\")\n  \n    payload = json.dumps({\n             \"q\": searchstatement\n         })\n    headers = {\n         'X-API-KEY':os.environ[\"SERPER_API_KEY\"],\n         'Content-Type': 'application/json'\n     }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    return response.text\n\n    # response = f'Todays date is {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n    # return response\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")",
  "app/llm_functions/tools/sqlite_tool.py": "\"\"\"\nSQLite Tool - Execute SQLite queries on the application database\n\"\"\"\n\nimport sqlite3\nfrom typing import Any, Dict, List\nfrom app.mcp.base_tool import BaseTool\n\n\n\nclass Sqlite(BaseTool):\n    \"\"\"Tool to execute SQLite queries\"\"\"\n\n    async def execute(self, query: str, limit: int = 100) -> Dict[str, Any]:\n        \"\"\"\n        Execute a SQLite query\n        \n        Args:\n            query: SQL query to execute\n            limit: Maximum number of results to return\n            \n        Returns:\n            Dictionary with query results\n        \"\"\"\n        try:\n            db_path = self.config.get(\"DB_PATH\", \"./my_database.db\")\n            timeout = self.config.get(\"TIMEOUT\", 30)\n            max_results = self.config.get(\"MAX_RESULTS\", 100)\n            \n            # Enforce limit\n            actual_limit = min(limit, max_results)\n            \n            # Add LIMIT clause if not present and it's a SELECT query\n            if query.strip().upper().startswith(\"SELECT\"):\n                if \"LIMIT\" not in query.upper():\n                    query += f\" LIMIT {actual_limit}\"\n            \n            logger.info(f\"Executing SQLite query: {query[:100]}...\")\n            \n            conn = sqlite3.connect(db_path, timeout=timeout)\n            conn.row_factory = sqlite3.Row\n            cursor = conn.cursor()\n            \n            cursor.execute(query)\n            \n            # For SELECT queries, fetch results\n            if query.strip().upper().startswith(\"SELECT\"):\n                rows = cursor.fetchall()\n                results = [dict(row) for row in rows]\n                \n                conn.close()\n                \n                return {\n                    \"success\": True,\n                    \"rows_affected\": len(results),\n                    \"data\": results,\n                    \"query\": query\n                }\n            else:\n                # For INSERT, UPDATE, DELETE queries\n                conn.commit()\n                rows_affected = cursor.rowcount\n                conn.close()\n                \n                return {\n                    \"success\": True,\n                    \"rows_affected\": rows_affected,\n                    \"data\": None,\n                    \"query\": query\n                }\n            \n        except sqlite3.Error as e:\n            logger.error(f\"SQLite error: {str(e)}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"query\": query\n            }\n        except Exception as e:\n            logger.error(f\"Error executing SQLite query: {str(e)}\")\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"query\": query\n            }\n\n    def get_schema(self) -> Dict[str, Any]:\n        \"\"\"Get input schema for Sqlite tool\"\"\"\n        return {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"SQL query to execute (SELECT, INSERT, UPDATE, DELETE)\"\n                },\n                \"limit\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Maximum number of results to return (for SELECT queries)\",\n                    \"default\": 100\n                }\n            },\n            \"required\": [\"query\"]\n        }\n",
  "app/llm_functions/tools2/current_date.py": "\nfrom langchain.tools import tool\nfrom datetime import datetime,UTC\nimport json\n\n@tool\ndef currentDate() -> str:\n    \"\"\"Used to get today's date, current date, current time etc. Should be used to get the date when user asks for current information. Date returned is in UTC format.\n    \"\"\"\n    now = datetime.now(UTC)\n    result = {\n        \"timestamp\": now.isoformat(),\n        \"unix_timestamp\": int(now.timestamp()),\n    }\n    \n    return json.dumps(result)\n\n\n",
  "app/llm_functions/tools2/google_search.py": "\nfrom langchain.tools import tool\nimport requests\nimport json\nimport os\nfrom datetime import datetime\nurl = \"https://google.serper.dev/search\"\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n@tool()\ndef search(searchstatement:str) -> str:\n    \"\"\"Used to search the web to get information \n        args:\n        searchstatement: The information being searched for in the web\n    \"\"\"\n   \n    if not \"SERPER_API_KEY\" in os.environ:\n         raise Exception(\"Serper API key is not defined\")\n  \n    payload = json.dumps({\n             \"q\": searchstatement\n         })\n    headers = {\n         'X-API-KEY':os.environ[\"SERPER_API_KEY\"],\n         'Content-Type': 'application/json'\n     }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    return response.text\n\n    # response = f'Todays date is {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n    # return response",
  "app/llm_functions/tools2/sqlite_tool.py": "",
  "app/llm_functions/tools2/toolsconfig.py": "from .current_date import currentDate\nfrom .google_search import search\n\ntoolsConfig=[{\n    \"key\":\"CurrentDate\",\n    \"value\":currentDate\n},\n{\n    \"key\":\"Search\",\n    \"value\":search\n}]",
  "app/middleware/auth_middleware.py": "\"\"\"\nAuthentication middleware\n\"\"\"\n\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom app.features.auth.jwt import get_user_id_from_token, create_access_token\nfrom app.core.config import settings\nfrom app.core.utils import get_logger\nfrom app.features.users.user_repository import UserRepository\n\n\nlogger = get_logger(__name__)\n\n\nclass AuthMiddleware(BaseHTTPMiddleware):\n    \"\"\"Middleware to validate authentication tokens.\"\"\"\n\n    async def dispatch(self, request: Request, call_next):\n        \"\"\"\n        Process request and validate token if needed.\n\n        Args:\n            request: HTTP request\n            call_next: Next middleware/route\n\n        Returns:\n            Response\n        \"\"\"\n        # Allow OPTIONS requests (CORS preflight) to pass through without authentication\n        if request.method == \"OPTIONS\":\n            return await call_next(request)\n        \n        # Check if route is excluded from authentication\n        path = request.url.path\n        if self._is_excluded_route(path):\n            logger.debug(f\"Public route accessed: {path}\")\n            return await call_next(request)\n\n        # Get token from header\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header:\n            logger.warning(f\"No authorization header for route: {path}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Missing authorization header\"},\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n\n        # Extract token\n        try:\n            scheme, token = auth_header.split()\n            if scheme.lower() != \"bearer\":\n                raise ValueError(\"Invalid scheme\")\n        except ValueError:\n            logger.warning(f\"Invalid authorization header format: {auth_header}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Invalid authorization header format\"},\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n\n        # Verify token\n        user_id = get_user_id_from_token(token)\n        if not user_id:\n            logger.warning(f\"Invalid token provided for route: {path}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Invalid or expired token\"},\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n\n        # Add user to request state\n        repo = UserRepository()\n        user = repo.get_by_id(user_id)\n        if not user or not user.is_active:\n            logger.warning(f\"Inactive or missing user for token: {user_id}\")\n            return JSONResponse(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                content={\"detail\": \"Inactive or missing user\"},\n                headers={\"WWW-Authenticate\": \"Bearer\"},\n            )\n        request.state.user = user\n        logger.debug(f\"Authenticated user {user.username} accessing {path}\")\n\n        response = await call_next(request)\n\n        # Check if user is authenticated and response is successful\n        if hasattr(request.state, \"user\") and 200 <= response.status_code < 300:\n            try:\n                # Create new token for the user\n                new_token = create_access_token(data={\"sub\": request.state.user.id,\"username\": user.username,\"role\": user.role})\n                # Add token to response headers\n                response.headers[\"X-Access-Token\"] = new_token\n            except Exception as e:\n                logger.error(f\"Failed to add refresh token: {str(e)}\")\n\n        return response\n\n    @staticmethod\n    def _is_excluded_route(path: str) -> bool:\n        \"\"\"\n        Check if route is excluded from authentication.\n\n        Args:\n            path: Request path\n\n        Returns:\n            True if route is excluded\n        \"\"\"\n        excluded = settings.auth_excluded_routes\n        for excluded_route in excluded:\n            if path == excluded_route or path.startswith(excluded_route):\n                return True\n        return False\n",
  "app/middleware/__init__.py": "\"\"\"\nMiddleware Module - Global middleware\n\"\"\"\n\nfrom .auth_middleware import AuthMiddleware\n\n__all__ = [\"AuthMiddleware\"]\n"
}

class ProjectRestorer:
    """Restores the project structure and content."""
    
    def __init__(self, base_path="."):
        self.base_path = Path(base_path)
        self.created_files = 0
        self.skipped_files = 0
        self.errors = 0

    def restore(self):
        for file_path_str, content in FILE_CONTENTS.items():
            self.create_file(file_path_str, content)

    def create_file(self, file_path_str, content):
        try:
            file_path = self.base_path / Path(file_path_str)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            self.created_files += 1
            
        except Exception as e:
            self.errors += 1

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Restore project files.")
    parser.add_argument("--path", default=".", help="Target directory (default: current)")
    args = parser.parse_args()
    restorer = ProjectRestorer(args.path)
    restorer.restore()

if __name__ == "__main__":
    main()
